{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "dqn.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/komo135/tradingrl/blob/master/dqn2.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "xluJbvKFSQsW",
        "outputId": "605d517e-37e9-43db-e334-de3206ea4e5b",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        }
      },
      "source": [
        "# Google ドライブをマウントするには、このセルを実行してください。\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "%cd drive/My Drive\n",
        "%load_ext Cython"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n",
            "/content/drive/My Drive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "asRYJu49SQsJ",
        "outputId": "231a89f5-14e8-4e94-8ca2-2a2270555a30",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 170
        }
      },
      "source": [
        "!pip install ta"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: ta in /usr/local/lib/python3.6/dist-packages (0.4.7)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.6/dist-packages (from ta) (0.25.2)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.6/dist-packages (from ta) (0.21.3)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (from ta) (1.17.3)\n",
            "Requirement already satisfied: python-dateutil>=2.6.1 in /usr/local/lib/python3.6/dist-packages (from pandas->ta) (2.6.1)\n",
            "Requirement already satisfied: pytz>=2017.2 in /usr/local/lib/python3.6/dist-packages (from pandas->ta) (2018.9)\n",
            "Requirement already satisfied: joblib>=0.11 in /usr/local/lib/python3.6/dist-packages (from scikit-learn->ta) (0.14.0)\n",
            "Requirement already satisfied: scipy>=0.17.0 in /usr/local/lib/python3.6/dist-packages (from scikit-learn->ta) (1.3.1)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.6/dist-packages (from python-dateutil>=2.6.1->pandas->ta) (1.12.0)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "2ArBkEJtSQrw",
        "outputId": "521f5495-2f6b-48a8-d3c8-46ceddcb1731",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 63
        }
      },
      "source": [
        "import numpy as np\n",
        "from scipy.special import expit\n",
        "import pandas as pd\n",
        "import pickle\n",
        "from numba import jit as njit\n",
        "from functools import lru_cache\n",
        "from multiprocessing import Pool\n",
        "import os\n",
        "import time\n",
        "import random\n",
        "import ta\n",
        "from net import *\n",
        "from memory import *\n",
        "from reward import *\n",
        "import traceback\n",
        "from IPython.display import clear_output"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "<p style=\"color: red;\">\n",
              "The default version of TensorFlow in Colab will soon switch to TensorFlow 2.x.<br>\n",
              "We recommend you <a href=\"https://www.tensorflow.org/guide/migrate\" target=\"_blank\">upgrade</a> now \n",
              "or ensure your notebook will continue to use TensorFlow 1.x via the <code>%tensorflow_version 1.x</code> magic:\n",
              "<a href=\"https://colab.research.google.com/notebooks/tensorflow_version.ipynb\" target=\"_blank\">more info</a>.</p>\n"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "yuYMpdBjSQrF",
        "colab": {}
      },
      "source": [
        "def swish(x):\n",
        "    x *= tf.nn.sigmoid(x)\n",
        "    return x\n",
        "\n",
        "def softmax(x):\n",
        "    \"\"\"Compute softmax values for each sets of scores in x.\"\"\"\n",
        "    e_x = np.exp(x - np.max(x))\n",
        "    return e_x / e_x.sum()\n",
        "\n",
        "def net(x,layer):\n",
        "  for i in layer:\n",
        "    x = i(x)\n",
        "  return x\n",
        "\n",
        "def actor(x,action):\n",
        "  with tf.variable_scope(\"actor\", reuse=False):\n",
        "    inputs = x\n",
        "    with tf.variable_scope(\"main\", reuse=False):\n",
        "      x1 = tf.keras.layers.Conv1D(128,8,1,padding=\"causal\",activation=swish)(x)\n",
        "      x2 = tf.keras.layers.Conv1D(128,4,1,padding=\"causal\",activation=swish)(x)\n",
        "      x3 = tf.keras.layers.Conv1D(128,2,1,padding=\"causal\",activation=swish)(x)\n",
        "      x = tf.keras.layers.Concatenate()([x1,x2,x3])\n",
        "      x = tf.keras.layers.Conv1D(128,3,1,padding=\"valid\",activation=swish)(x)\n",
        "      f = tf.keras.layers.Flatten()(x)\n",
        "    with tf.variable_scope(\"sub\", reuse=False):\n",
        "      x = tf.keras.layers.Flatten()(inputs)\n",
        "      shape = int(x.shape[-1])\n",
        "      a = tf.keras.layers.Dense(shape,tf.nn.softmax)(f)\n",
        "      mul = tf.keras.layers.Multiply()([x,a])\n",
        "      out = tf.keras.layers.Dense(128,swish)(mul)\n",
        "      out = tf.keras.layers.Dense(3,tf.tanh)(out)\n",
        "    with tf.variable_scope(\"main2\", reuse=False):\n",
        "      # x = tf.keras.layers.Dense(128,tf.nn.relu)(f)\n",
        "      x1 = tf.keras.layers.Concatenate()([out,f])\n",
        "      x2 = tf.keras.layers.Concatenate()([action,f])\n",
        "      layer = [tf.keras.layers.Dense(128,tf.nn.relu),tf.keras.layers.Dense(3)]\n",
        "      x1 = net(x1,layer)\n",
        "      x2 = net(x2,layer)\n",
        "      X = tf.keras.layers.Maximum()([x1[:,0],x1[:,1],x1[:,2]])\n",
        "  return X,x1,x2,out"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mnaWdzaJY8tS",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class Agent:\n",
        "    def __init__(self,path,window_siz):\n",
        "      self.path = path\n",
        "      self.window_size = window_size\n",
        "      self.STEP_SIZE = 20 * 12\n",
        "      self.preproc()\n",
        "      self.state_size = (None,self.window_size,self.df.shape[-1])\n",
        "      print(self.state_size)\n",
        "      self.memory = Memory(5000)\n",
        "      self.rewards = reward3\n",
        "      self.noise = 0.3\n",
        "      self.noise_min = 0.1\n",
        "      self.sess = tf.Session()\n",
        "\n",
        "      with tf.variable_scope(\"input\"):\n",
        "        self.state = tf.placeholder(tf.float32, self.state_size)\n",
        "        self.new_state = tf.placeholder(tf.float32, self.state_size)\n",
        "        self.action = tf.placeholder(tf.float32,(None,3))\n",
        "        self.reward = tf.placeholder(tf.float32,(None,3))\n",
        "\n",
        "      with tf.variable_scope(\"model\", reuse=False):\n",
        "        self.policy,self.q_pi,self.q,self.out = actor(self.state,self.action)\n",
        "      \n",
        "      with tf.variable_scope(\"target\", reuse=False):\n",
        "        _,self.target_q_pi,_,_ = actor(self.new_state,self.action)\n",
        "\n",
        "      with tf.variable_scope(\"loss\"):\n",
        "        self.loss = loss = (0.5 * tf.reduce_mean((self.reward - self.q) ** 2))\n",
        "        self.ploss = policy_loss = -tf.reduce_mean(self.policy)\n",
        "        self.absolute_errors = tf.abs(self.reward - self.q)\n",
        "\n",
        "      self.policy_sub_opt = tf.train.AdamOptimizer(1e-4).minimize(policy_loss, var_list=get_vars('model/actor/sub'))\n",
        "      self.policy_main_opt = tf.train.AdamOptimizer(1e-3).minimize(loss, var_list=get_vars('model/actor/main')+get_vars('model/actor/main2'))\n",
        "\n",
        "      self.target_update = tf.group([tf.assign(v_targ, 0.001*v_targ + (1-0.001)*v_main)\n",
        "                                for v_main, v_targ in zip(get_vars('model'), get_vars('target'))])\n",
        "\n",
        "      target_init = tf.group([tf.assign(v_targ, v_main)\n",
        "                              for v_main, v_targ in zip(get_vars('model'), get_vars('target'))])\n",
        "\n",
        "      self.sess.run(tf.global_variables_initializer())\n",
        "      # self.sess.run(target_init)\n",
        "    def preproc(self):\n",
        "        self.dat = df = pd.read_csv(self.path)\n",
        "        s = np.asanyarray(ta.stoch(df[\"High\"],df[\"Low\"],df[\"Close\"],14)).reshape((-1, 1)) - np.asanyarray(ta.stoch_signal(df[\"High\"],df[\"Low\"],df[\"Close\"],14)).reshape((-1, 1))\n",
        "        x = np.asanyarray(ta.daily_return(df[\"Close\"])).reshape((-1,1))\n",
        "        m = np.asanyarray(ta.macd_diff(df[\"Close\"])).reshape((-1,1))\n",
        "        cross1 = np.asanyarray(ta.ema(self.dat[\"Close\"],12)).reshape((-1, 1)) - np.asanyarray(ta.ema(self.dat[\"Close\"],5)).reshape((-1, 1))\n",
        "        trend = np.asanyarray(df[[\"Close\"]]) - np.asanyarray(ta.ema(self.dat[\"Close\"],50)).reshape((-1, 1))\n",
        "        # x = s\n",
        "        x = np.concatenate([s,x,cross1,trend,m], 1)\n",
        "        y = np.asanyarray(self.dat[[\"Open\"]])\n",
        "\n",
        "        gen = tf.keras.preprocessing.sequence.TimeseriesGenerator(x, y, self.window_size)\n",
        "        self.x = []\n",
        "        self.y = []\n",
        "        for i in gen:\n",
        "            self.x.extend(i[0].tolist())\n",
        "            self.y.extend(i[1].tolist())\n",
        "        self.x = np.asanyarray(self.x)[100:] #.reshape((-1, self.window_size, x.shape[-1]))\n",
        "        self.y = np.asanyarray(self.y)[100:]\n",
        "\n",
        "        self.df = self.x[-self.STEP_SIZE:]\n",
        "        self.trend = self.y[-self.STEP_SIZE:]\n",
        "\n",
        "    def _construct_memories_and_train(self,i, replay=None):\n",
        "      try:\n",
        "          tree_idx, replay = self.memory.sample(128)\n",
        "      except:\n",
        "          self.memory = Memory(5000)\n",
        "\n",
        "      states = np.array([a[0][0] for a in replay])\n",
        "      new_states = np.array([a[0][3] for a in replay])\n",
        "      actions = np.array([a[0][1] for a in replay]).reshape((-1, 3))\n",
        "      rewards = np.array([a[0][2] for a in replay]).reshape((-1, 1))\n",
        "      done = np.array([a[0][-1] for a in replay]).reshape((-1, 1))\n",
        "\n",
        "      # target_q = self.sess.run(self.target_q, feed_dict={self.new_state:new_states}).reshape((-1,1))\n",
        "      q = self.sess.run(self.q,feed_dict={self.state:states,self.action:actions})\n",
        "      # new_q = self.sess.run(self.q_pi,feed_dict={self.state:new_states})\n",
        "      # new_target_q = self.sess.run(self.target_q_pi, feed_dict={self.new_state:new_states})\n",
        "\n",
        "      for I in range(128):\n",
        "        q[I,np.argmax(actions[I])] = rewards[I]# + done[I] * 0.99 * new_target_q[I,np.argmax(new_q[I])]\n",
        "      # print(q)\n",
        "\n",
        "      step_ops = [self.absolute_errors,self.policy_main_opt]\n",
        "      for _ in range(1):\n",
        "        absolute_errors,_ = self.sess.run(step_ops, feed_dict={self.state: states, self.new_state: new_states, self.reward: q,self.action:actions})\n",
        "      if i > 10:\n",
        "        if (i+1) % 2 == 0:\n",
        "              loss,_ = self.sess.run([self.loss,self.policy_sub_opt], feed_dict={self.state: states,self.reward: q,self.action:actions})\n",
        "              # print(loss)\n",
        "          # print([rewards,loss])\n",
        "      self.sess.run(self.target_update)\n",
        "      ae = []\n",
        "      for i in absolute_errors:\n",
        "        ae.append(np.mean(i))\n",
        "      ae = np.array(ae)\n",
        "      self.memory.batch_update(tree_idx, ae)\n",
        "\n",
        "    def _select_action(self, state, i):\n",
        "        # self.policy_out\n",
        "        prediction,q = self.sess.run([self.out,self.q_pi], feed_dict={self.state: [state]})\n",
        "        prediction,q = prediction[0],q[0]\n",
        "\n",
        "        if self.noise > self.noise_min:\n",
        "          self.noise *= 0.9999\n",
        "        noise = 0. if (i + 1) % 5 == 0 else self.noise\n",
        "        # print(prediction)\n",
        "        prediction += (noise * np.random.randn(3))\n",
        "        prediction = np.clip(prediction, -1, 1)\n",
        "        Q = q * np.clip(prediction, 0, 1)\n",
        "        # prediction = q\n",
        "        action = np.argmax(Q)\n",
        "        self.pred = prediction\n",
        "\n",
        "        return action\n",
        "\n",
        "    def prob(self,history):\n",
        "        prob = np.asanyarray(history)\n",
        "        a = np.mean(prob == 0)\n",
        "        b = np.mean(prob == 1)\n",
        "        c = 1 - (a + b)\n",
        "        prob = [a,b,c]\n",
        "        return prob\n",
        "\n",
        "    def discount_rewards(self, r, running_add):\n",
        "        running_add = running_add * 0.99 + r\n",
        "        return running_add\n",
        "        \n",
        "    def nstep(self,r):\n",
        "        running_add = 0.0\n",
        "        for t in range(len(r)):\n",
        "            running_add += 0.99 * r[t]\n",
        "\n",
        "        return running_add\n",
        "\n",
        "    def run(self, spread=10, pip_cost=1000, los_cut=600, day_pip=20, n=10,step=100):\n",
        "        spread = spread / pip_cost\n",
        "        self.rand = np.random.RandomState()\n",
        "        lc = los_cut / pip_cost\n",
        "        for i in range(100000):\n",
        "            # if (i - 1) % step == 0:\n",
        "            # h = self.rand.randint(self.x.shape[0]-(self.STEP_SIZE+1))\n",
        "            # self.df = self.x[h:h+self.STEP_SIZE]\n",
        "            # self.trend = self.y[h:h+self.STEP_SIZE]\n",
        "            done = 1.0\n",
        "            position = 3\n",
        "            pip = []\n",
        "            provisional_pip = []\n",
        "            running_add = 0.0\n",
        "            total_pip = 0.0\n",
        "            old_reword = 0.0\n",
        "            states = []\n",
        "            h_a = []\n",
        "            h_r = []\n",
        "            h_p = []\n",
        "            old = np.asanyarray(0)\n",
        "            self.history = []\n",
        "            for t in  range(0, len(self.trend)-1):\n",
        "                action = self._select_action(self.df[t],i)\n",
        "                h_a.append(self.pred)\n",
        "                self.history.append(action)\n",
        "                \n",
        "                states,provisional_pip,position,total_pip = self.rewards(self.trend[t],pip,provisional_pip,\n",
        "                                    action,position,states,pip_cost,spread,total_pip,lc=min([lc/2,100/pip_cost]))\n",
        "                h_p.append(position)\n",
        "                \n",
        "                # r_d = np.mean(np.array(provisional_pip) > 0) * 100 if len(provisional_pip) != 0 else 0.\n",
        "                # reward =  r_d\n",
        "                reward =  (total_pip - old_reword) * 100\n",
        "                old_reword = total_pip\n",
        "                h_r.append(reward)\n",
        "\n",
        "                # running_add = self.discount_rewards(reward,running_add)\n",
        "                # r_d = int(running_add * (pip_cost / 100)) * 100\n",
        "                # # r_d = int(np.mean(np.array(provisional_pip) > 0)*100) if len(provisional_pip) != 0 else 0.\n",
        "                # if t == len(self.trend)-5:\n",
        "                #     done = 0.\n",
        "                # exp = self.df[t], h_a[t], r_d, self.df[t+1], done\n",
        "                # self.memory.store(exp)\n",
        "            for t in range(0, len(self.trend)-1):\n",
        "                tau = t - n + 1\n",
        "                if tau >= 0:\n",
        "                    rewards = self.nstep(h_r[tau+1:tau+n])\n",
        "                    exp = self.df[t], h_a[t], int(rewards), self.df[t+1], done\n",
        "                    self.memory.store(exp)\n",
        "\n",
        "            try:\n",
        "              self._construct_memories_and_train(i)\n",
        "            except:\n",
        "              traceback.print_exc()\n",
        "\n",
        "            if i % 1000 == 0:\n",
        "              clear_output()\n",
        "\n",
        "            if (i + 1) % 5 == 0:\n",
        "                # loss = np.mean(ae)\n",
        "                self.pip = np.asanyarray(provisional_pip) * pip_cost\n",
        "                self.pip = [p if p >= -los_cut else -los_cut for p in self.pip]\n",
        "                self.total_pip = np.sum(self.pip)\n",
        "                mean_pip = self.total_pip / (t + 1)\n",
        "                trade_accuracy = np.mean(np.asanyarray(self.pip) > 0)\n",
        "                self.trade = trade_accuracy\n",
        "                mean_pip *= day_pip\n",
        "                prob = self.prob(self.history)\n",
        "                position_prob = self.prob(h_p)\n",
        "\n",
        "                # print(\"loss =\", loss)\n",
        "                # print(\"\")\n",
        "                print('action probability = ', prob)\n",
        "                print(\"buy = \", position_prob[1], \" sell = \", position_prob[-1])\n",
        "                print('trade accuracy = ', trade_accuracy)\n",
        "                print('epoch: %d, total rewards: %f, mean rewards: %f' % (i + 1, float(self.total_pip), float(mean_pip)))\n",
        "                print(\"\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hFioYWzBenRt",
        "colab_type": "code",
        "outputId": "2efb3aff-8939-474b-d061-ca2ead4f69cc",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "window_size = 30\n",
        "path = \"audpred1440.csv\"\n",
        "agent = Agent(path,window_size)\n",
        "\n",
        "agent.run()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "action probability =  [0.8786610878661087, 0.10460251046025104, 0.01673640167364021]\n",
            "buy =  0.891213389121339  sell =  0.10878661087866104\n",
            "trade accuracy =  0.3407079646017699\n",
            "epoch: 5, total rewards: 39649.000000, mean rewards: 3317.907950\n",
            "\n",
            "action probability =  [0.6778242677824268, 0.2217573221757322, 0.10041841004184104]\n",
            "buy =  0.7531380753138075  sell =  0.2468619246861925\n",
            "trade accuracy =  0.4523809523809524\n",
            "epoch: 10, total rewards: 95781.000000, mean rewards: 8015.146444\n",
            "\n",
            "action probability =  [0.6569037656903766, 0.23430962343096234, 0.10878661087866104]\n",
            "buy =  0.7238493723849372  sell =  0.2761506276150628\n",
            "trade accuracy =  0.49504950495049505\n",
            "epoch: 15, total rewards: 81953.000000, mean rewards: 6857.991632\n",
            "\n",
            "action probability =  [0.5481171548117155, 0.26778242677824265, 0.18410041841004188]\n",
            "buy =  0.6652719665271967  sell =  0.33472803347280333\n",
            "trade accuracy =  0.4808743169398907\n",
            "epoch: 20, total rewards: 65908.000000, mean rewards: 5515.313808\n",
            "\n",
            "action probability =  [0.502092050209205, 0.1799163179916318, 0.31799163179916323]\n",
            "buy =  0.7112970711297071  sell =  0.2887029288702929\n",
            "trade accuracy =  0.4144736842105263\n",
            "epoch: 25, total rewards: 50137.000000, mean rewards: 4195.564854\n",
            "\n",
            "action probability =  [0.38493723849372385, 0.301255230125523, 0.3138075313807531]\n",
            "buy =  0.5690376569037657  sell =  0.43096234309623427\n",
            "trade accuracy =  0.39622641509433965\n",
            "epoch: 30, total rewards: 25350.000000, mean rewards: 2121.338912\n",
            "\n",
            "action probability =  [0.48535564853556484, 0.2719665271966527, 0.2426778242677825]\n",
            "buy =  0.5815899581589958  sell =  0.41841004184100417\n",
            "trade accuracy =  0.4069767441860465\n",
            "epoch: 35, total rewards: 22187.000000, mean rewards: 1856.652720\n",
            "\n",
            "action probability =  [0.40585774058577406, 0.2510460251046025, 0.34309623430962344]\n",
            "buy =  0.6192468619246861  sell =  0.38075313807531386\n",
            "trade accuracy =  0.3877551020408163\n",
            "epoch: 40, total rewards: 5702.000000, mean rewards: 477.154812\n",
            "\n",
            "action probability =  [0.5146443514644351, 0.2928870292887029, 0.19246861924686198]\n",
            "buy =  0.606694560669456  sell =  0.39330543933054396\n",
            "trade accuracy =  0.398876404494382\n",
            "epoch: 45, total rewards: 9038.000000, mean rewards: 756.317992\n",
            "\n",
            "action probability =  [0.5439330543933054, 0.30962343096234307, 0.14644351464435146]\n",
            "buy =  0.606694560669456  sell =  0.39330543933054396\n",
            "trade accuracy =  0.37305699481865284\n",
            "epoch: 50, total rewards: 5090.000000, mean rewards: 425.941423\n",
            "\n",
            "action probability =  [0.4686192468619247, 0.2510460251046025, 0.2803347280334728]\n",
            "buy =  0.6150627615062761  sell =  0.38493723849372385\n",
            "trade accuracy =  0.38509316770186336\n",
            "epoch: 55, total rewards: 13263.000000, mean rewards: 1109.874477\n",
            "\n",
            "action probability =  [0.5188284518828452, 0.28451882845188287, 0.19665271966527187]\n",
            "buy =  0.602510460251046  sell =  0.39748953974895396\n",
            "trade accuracy =  0.3770491803278688\n",
            "epoch: 60, total rewards: 15281.000000, mean rewards: 1278.744770\n",
            "\n",
            "action probability =  [0.39330543933054396, 0.4309623430962343, 0.17573221757322166]\n",
            "buy =  0.4476987447698745  sell =  0.5523012552301255\n",
            "trade accuracy =  0.42328042328042326\n",
            "epoch: 65, total rewards: 39509.000000, mean rewards: 3306.192469\n",
            "\n",
            "action probability =  [0.3514644351464435, 0.5188284518828452, 0.12970711297071125]\n",
            "buy =  0.37656903765690375  sell =  0.6234309623430963\n",
            "trade accuracy =  0.4120603015075377\n",
            "epoch: 70, total rewards: 40004.000000, mean rewards: 3347.615063\n",
            "\n",
            "action probability =  [0.20502092050209206, 0.6443514644351465, 0.15062761506276146]\n",
            "buy =  0.2301255230125523  sell =  0.7698744769874477\n",
            "trade accuracy =  0.3939393939393939\n",
            "epoch: 75, total rewards: 28010.000000, mean rewards: 2343.933054\n",
            "\n",
            "action probability =  [0.16736401673640167, 0.698744769874477, 0.13389121338912136]\n",
            "buy =  0.1799163179916318  sell =  0.8200836820083682\n",
            "trade accuracy =  0.38308457711442784\n",
            "epoch: 80, total rewards: 41251.000000, mean rewards: 3451.966527\n",
            "\n",
            "action probability =  [0.11715481171548117, 0.6443514644351465, 0.2384937238493724]\n",
            "buy =  0.14644351464435146  sell =  0.8535564853556485\n",
            "trade accuracy =  0.3333333333333333\n",
            "epoch: 85, total rewards: 12487.000000, mean rewards: 1044.937238\n",
            "\n",
            "action probability =  [0.24686192468619247, 0.606694560669456, 0.14644351464435146]\n",
            "buy =  0.26778242677824265  sell =  0.7322175732217573\n",
            "trade accuracy =  0.4120603015075377\n",
            "epoch: 90, total rewards: 45799.000000, mean rewards: 3832.552301\n",
            "\n",
            "action probability =  [0.30962343096234307, 0.5983263598326359, 0.09205020920502105]\n",
            "buy =  0.3263598326359833  sell =  0.6736401673640167\n",
            "trade accuracy =  0.4380952380952381\n",
            "epoch: 95, total rewards: 47123.000000, mean rewards: 3943.347280\n",
            "\n",
            "action probability =  [0.39748953974895396, 0.3054393305439331, 0.2970711297071129]\n",
            "buy =  0.5439330543933054  sell =  0.4560669456066946\n",
            "trade accuracy =  0.3619631901840491\n",
            "epoch: 100, total rewards: 46116.000000, mean rewards: 3859.079498\n",
            "\n",
            "action probability =  [0.497907949790795, 0.1799163179916318, 0.32217573221757323]\n",
            "buy =  0.6610878661087866  sell =  0.33891213389121344\n",
            "trade accuracy =  0.3974358974358974\n",
            "epoch: 105, total rewards: 33998.000000, mean rewards: 2845.020921\n",
            "\n",
            "action probability =  [0.3305439330543933, 0.45188284518828453, 0.2175732217573222]\n",
            "buy =  0.38493723849372385  sell =  0.6150627615062761\n",
            "trade accuracy =  0.4147727272727273\n",
            "epoch: 110, total rewards: 72301.000000, mean rewards: 6050.292887\n",
            "\n",
            "action probability =  [0.37656903765690375, 0.5313807531380753, 0.09205020920502094]\n",
            "buy =  0.39748953974895396  sell =  0.602510460251046\n",
            "trade accuracy =  0.46190476190476193\n",
            "epoch: 115, total rewards: 58972.000000, mean rewards: 4934.895397\n",
            "\n",
            "action probability =  [0.28451882845188287, 0.35564853556485354, 0.35983263598326354]\n",
            "buy =  0.39748953974895396  sell =  0.602510460251046\n",
            "trade accuracy =  0.36\n",
            "epoch: 120, total rewards: 15037.000000, mean rewards: 1258.326360\n",
            "\n",
            "action probability =  [0.40585774058577406, 0.28451882845188287, 0.3096234309623431]\n",
            "buy =  0.5104602510460251  sell =  0.4895397489539749\n",
            "trade accuracy =  0.33962264150943394\n",
            "epoch: 125, total rewards: 13253.000000, mean rewards: 1109.037657\n",
            "\n",
            "action probability =  [0.497907949790795, 0.46443514644351463, 0.03765690376569042]\n",
            "buy =  0.5062761506276151  sell =  0.4937238493723849\n",
            "trade accuracy =  0.4343891402714932\n",
            "epoch: 130, total rewards: 45479.000000, mean rewards: 3805.774059\n",
            "\n",
            "action probability =  [0.4476987447698745, 0.2301255230125523, 0.32217573221757323]\n",
            "buy =  0.5815899581589958  sell =  0.41841004184100417\n",
            "trade accuracy =  0.3717948717948718\n",
            "epoch: 135, total rewards: 24248.000000, mean rewards: 2029.121339\n",
            "\n",
            "action probability =  [0.4435146443514644, 0.3514644351464435, 0.20502092050209209]\n",
            "buy =  0.5313807531380753  sell =  0.4686192468619247\n",
            "trade accuracy =  0.3891891891891892\n",
            "epoch: 140, total rewards: 34645.000000, mean rewards: 2899.163180\n",
            "\n",
            "action probability =  [0.48535564853556484, 0.41422594142259417, 0.10041841004184104]\n",
            "buy =  0.497907949790795  sell =  0.502092050209205\n",
            "trade accuracy =  0.4097560975609756\n",
            "epoch: 145, total rewards: 35535.000000, mean rewards: 2973.640167\n",
            "\n",
            "action probability =  [0.41841004184100417, 0.4435146443514644, 0.13807531380753146]\n",
            "buy =  0.4602510460251046  sell =  0.5397489539748954\n",
            "trade accuracy =  0.398989898989899\n",
            "epoch: 150, total rewards: 41148.000000, mean rewards: 3443.347280\n",
            "\n",
            "action probability =  [0.5983263598326359, 0.22594142259414227, 0.17573221757322177]\n",
            "buy =  0.698744769874477  sell =  0.301255230125523\n",
            "trade accuracy =  0.35106382978723405\n",
            "epoch: 155, total rewards: 17289.000000, mean rewards: 1446.778243\n",
            "\n",
            "action probability =  [0.42677824267782427, 0.33472803347280333, 0.2384937238493724]\n",
            "buy =  0.5188284518828452  sell =  0.4811715481171548\n",
            "trade accuracy =  0.38372093023255816\n",
            "epoch: 160, total rewards: 25043.000000, mean rewards: 2095.648536\n",
            "\n",
            "action probability =  [0.4811715481171548, 0.3891213389121339, 0.12970711297071125]\n",
            "buy =  0.5355648535564853  sell =  0.4644351464435147\n",
            "trade accuracy =  0.37948717948717947\n",
            "epoch: 165, total rewards: 32143.000000, mean rewards: 2689.790795\n",
            "\n",
            "action probability =  [0.5355648535564853, 0.39748953974895396, 0.06694560669456073]\n",
            "buy =  0.5732217573221757  sell =  0.42677824267782427\n",
            "trade accuracy =  0.3886255924170616\n",
            "epoch: 170, total rewards: 41363.000000, mean rewards: 3461.338912\n",
            "\n",
            "action probability =  [0.5146443514644351, 0.28870292887029286, 0.1966527196652721]\n",
            "buy =  0.602510460251046  sell =  0.39748953974895396\n",
            "trade accuracy =  0.40437158469945356\n",
            "epoch: 175, total rewards: 44801.000000, mean rewards: 3749.037657\n",
            "\n",
            "action probability =  [0.5062761506276151, 0.2510460251046025, 0.2426778242677824]\n",
            "buy =  0.6276150627615062  sell =  0.37238493723849375\n",
            "trade accuracy =  0.3742690058479532\n",
            "epoch: 180, total rewards: 33118.000000, mean rewards: 2771.380753\n",
            "\n",
            "action probability =  [0.47280334728033474, 0.35564853556485354, 0.17154811715481166]\n",
            "buy =  0.5690376569037657  sell =  0.43096234309623427\n",
            "trade accuracy =  0.3870967741935484\n",
            "epoch: 185, total rewards: 41541.000000, mean rewards: 3476.234310\n",
            "\n",
            "action probability =  [0.5481171548117155, 0.3389121338912134, 0.11297071129707104]\n",
            "buy =  0.6150627615062761  sell =  0.38493723849372385\n",
            "trade accuracy =  0.3768844221105528\n",
            "epoch: 190, total rewards: 57245.000000, mean rewards: 4790.376569\n",
            "\n",
            "action probability =  [0.4393305439330544, 0.4769874476987448, 0.08368200836820083]\n",
            "buy =  0.46443514644351463  sell =  0.5355648535564854\n",
            "trade accuracy =  0.4057971014492754\n",
            "epoch: 195, total rewards: 76874.000000, mean rewards: 6432.970711\n",
            "\n",
            "action probability =  [0.49372384937238495, 0.34309623430962344, 0.16317991631799167]\n",
            "buy =  0.5815899581589958  sell =  0.41841004184100417\n",
            "trade accuracy =  0.39893617021276595\n",
            "epoch: 200, total rewards: 35611.000000, mean rewards: 2980.000000\n",
            "\n",
            "action probability =  [0.497907949790795, 0.3138075313807531, 0.18828451882845187]\n",
            "buy =  0.6192468619246861  sell =  0.38075313807531386\n",
            "trade accuracy =  0.37362637362637363\n",
            "epoch: 205, total rewards: 30275.000000, mean rewards: 2533.472803\n",
            "\n",
            "action probability =  [0.497907949790795, 0.3138075313807531, 0.18828451882845187]\n",
            "buy =  0.6108786610878661  sell =  0.38912133891213385\n",
            "trade accuracy =  0.40331491712707185\n",
            "epoch: 210, total rewards: 30217.000000, mean rewards: 2528.619247\n",
            "\n",
            "action probability =  [0.4811715481171548, 0.3389121338912134, 0.17991631799163188]\n",
            "buy =  0.5857740585774058  sell =  0.41422594142259417\n",
            "trade accuracy =  0.42777777777777776\n",
            "epoch: 215, total rewards: 47021.000000, mean rewards: 3934.811715\n",
            "\n",
            "action probability =  [0.4309623430962343, 0.42677824267782427, 0.14225941422594146]\n",
            "buy =  0.502092050209205  sell =  0.497907949790795\n",
            "trade accuracy =  0.4270833333333333\n",
            "epoch: 220, total rewards: 56837.000000, mean rewards: 4756.234310\n",
            "\n",
            "action probability =  [0.45188284518828453, 0.4100418410041841, 0.13807531380753135]\n",
            "buy =  0.5230125523012552  sell =  0.4769874476987448\n",
            "trade accuracy =  0.4489795918367347\n",
            "epoch: 225, total rewards: 59101.000000, mean rewards: 4945.690377\n",
            "\n",
            "action probability =  [0.4309623430962343, 0.40585774058577406, 0.16317991631799167]\n",
            "buy =  0.5439330543933054  sell =  0.4560669456066946\n",
            "trade accuracy =  0.4444444444444444\n",
            "epoch: 230, total rewards: 64470.000000, mean rewards: 5394.979079\n",
            "\n",
            "action probability =  [0.5271966527196653, 0.3054393305439331, 0.16736401673640167]\n",
            "buy =  0.6192468619246861  sell =  0.38075313807531386\n",
            "trade accuracy =  0.35294117647058826\n",
            "epoch: 235, total rewards: 20392.000000, mean rewards: 1706.443515\n",
            "\n",
            "action probability =  [0.5271966527196653, 0.26359832635983266, 0.20920502092050208]\n",
            "buy =  0.5983263598326359  sell =  0.40167364016736407\n",
            "trade accuracy =  0.33519553072625696\n",
            "epoch: 240, total rewards: 21140.000000, mean rewards: 1769.037657\n",
            "\n",
            "action probability =  [0.4100418410041841, 0.5355648535564853, 0.05439330543933063]\n",
            "buy =  0.4351464435146444  sell =  0.5648535564853556\n",
            "trade accuracy =  0.40186915887850466\n",
            "epoch: 245, total rewards: 65295.000000, mean rewards: 5464.016736\n",
            "\n",
            "action probability =  [0.4769874476987448, 0.4686192468619247, 0.05439330543933052]\n",
            "buy =  0.49372384937238495  sell =  0.506276150627615\n",
            "trade accuracy =  0.42452830188679247\n",
            "epoch: 250, total rewards: 37240.000000, mean rewards: 3116.317992\n",
            "\n",
            "action probability =  [0.4309623430962343, 0.4560669456066946, 0.11297071129707104]\n",
            "buy =  0.4811715481171548  sell =  0.5188284518828452\n",
            "trade accuracy =  0.4729064039408867\n",
            "epoch: 255, total rewards: 110050.000000, mean rewards: 9209.205021\n",
            "\n",
            "action probability =  [0.49372384937238495, 0.3807531380753138, 0.12552301255230125]\n",
            "buy =  0.5481171548117155  sell =  0.4518828451882845\n",
            "trade accuracy =  0.4494949494949495\n",
            "epoch: 260, total rewards: 84868.000000, mean rewards: 7101.924686\n",
            "\n",
            "action probability =  [0.401673640167364, 0.3723849372384937, 0.2259414225941423]\n",
            "buy =  0.4769874476987448  sell =  0.5230125523012552\n",
            "trade accuracy =  0.49444444444444446\n",
            "epoch: 265, total rewards: 131668.000000, mean rewards: 11018.242678\n",
            "\n",
            "action probability =  [0.38493723849372385, 0.25523012552301255, 0.35983263598326354]\n",
            "buy =  0.5774058577405857  sell =  0.4225941422594143\n",
            "trade accuracy =  0.4527027027027027\n",
            "epoch: 270, total rewards: 100440.000000, mean rewards: 8405.020921\n",
            "\n",
            "action probability =  [0.36401673640167365, 0.401673640167364, 0.2343096234309623]\n",
            "buy =  0.4351464435146444  sell =  0.5648535564853556\n",
            "trade accuracy =  0.45251396648044695\n",
            "epoch: 275, total rewards: 112189.000000, mean rewards: 9388.200837\n",
            "\n",
            "action probability =  [0.2510460251046025, 0.5857740585774058, 0.16317991631799167]\n",
            "buy =  0.301255230125523  sell =  0.698744769874477\n",
            "trade accuracy =  0.45077720207253885\n",
            "epoch: 280, total rewards: 78562.000000, mean rewards: 6574.225941\n",
            "\n",
            "action probability =  [0.47280334728033474, 0.1087866108786611, 0.41841004184100417]\n",
            "buy =  0.7782426778242678  sell =  0.22175732217573219\n",
            "trade accuracy =  0.31297709923664124\n",
            "epoch: 285, total rewards: 2828.000000, mean rewards: 236.652720\n",
            "\n",
            "action probability =  [0.39748953974895396, 0.1506276150627615, 0.4518828451882846]\n",
            "buy =  0.6234309623430963  sell =  0.37656903765690375\n",
            "trade accuracy =  0.448\n",
            "epoch: 290, total rewards: 72638.000000, mean rewards: 6078.493724\n",
            "\n",
            "action probability =  [0.42677824267782427, 0.2217573221757322, 0.35146443514644354]\n",
            "buy =  0.6234309623430963  sell =  0.37656903765690375\n",
            "trade accuracy =  0.41216216216216217\n",
            "epoch: 295, total rewards: 45279.000000, mean rewards: 3789.037657\n",
            "\n",
            "action probability =  [0.4476987447698745, 0.10460251046025104, 0.4476987447698745]\n",
            "buy =  0.7573221757322176  sell =  0.2426778242677824\n",
            "trade accuracy =  0.31451612903225806\n",
            "epoch: 300, total rewards: 4889.000000, mean rewards: 409.121339\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LYepHLr81cCD",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "try:\n",
        "    tree_idx, replay = agent.memory.sample(128)\n",
        "except:\n",
        "    agent.memory = Memory(5000)\n",
        "\n",
        "states = np.array([a[0][0] for a in replay])\n",
        "new_states = np.array([a[0][3] for a in replay])\n",
        "actions = np.array([a[0][1] for a in replay]).reshape((-1, 2))\n",
        "rewards = np.array([a[0][2] for a in replay]).reshape((-1, 1))\n",
        "done = np.array([a[0][-1] for a in replay]).reshape((-1, 1))\n",
        "\n",
        "# target_q = self.sess.run(self.target_q, feed_dict={self.new_state:new_states}).reshape((-1,1))\n",
        "q = agent.sess.run(agent.q,feed_dict={agent.state:states,agent.action:actions})\n",
        "new_q = agent.sess.run(agent.q_pi,feed_dict={agent.state:new_states})\n",
        "new_target_q = agent.sess.run(agent.target_q_pi, feed_dict={agent.new_state:new_states})\n",
        "\n",
        "for I in range(128):\n",
        "  q[I,np.argmax(actions[I])] = rewards[I] + done[I] * 0.99 * new_target_q[I,np.argmax(new_q[I])]\n",
        "# print(q)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gbnKMy0urf70",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "q[0:10]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zZUTUKDdrOC8",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "new_q[0:10]"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}