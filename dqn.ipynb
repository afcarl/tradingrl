{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "dqn.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/komo135/tradingrl/blob/master/dqn.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cBVV8hQizHTI",
        "colab_type": "code",
        "outputId": "0152be9e-8863-4455-f47a-b6d274537b72",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        }
      },
      "source": [
        "# Google ドライブをマウントするには、このセルを実行してください。\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "%cd drive/My Drive\n",
        "%load_ext Cython"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n",
            "/content/drive/My Drive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "prD6-LCFQYzW",
        "colab_type": "code",
        "outputId": "6fdea6b7-7def-4c5f-c27e-7a8246a8e322",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 170
        }
      },
      "source": [
        "!pip install ta"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: ta in /usr/local/lib/python3.6/dist-packages (0.4.7)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (from ta) (1.17.3)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.6/dist-packages (from ta) (0.25.2)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.6/dist-packages (from ta) (0.21.3)\n",
            "Requirement already satisfied: python-dateutil>=2.6.1 in /usr/local/lib/python3.6/dist-packages (from pandas->ta) (2.6.1)\n",
            "Requirement already satisfied: pytz>=2017.2 in /usr/local/lib/python3.6/dist-packages (from pandas->ta) (2018.9)\n",
            "Requirement already satisfied: scipy>=0.17.0 in /usr/local/lib/python3.6/dist-packages (from scikit-learn->ta) (1.3.1)\n",
            "Requirement already satisfied: joblib>=0.11 in /usr/local/lib/python3.6/dist-packages (from scikit-learn->ta) (0.14.0)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.6/dist-packages (from python-dateutil>=2.6.1->pandas->ta) (1.12.0)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rfuucG8nABQX",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import numpy as np\n",
        "from scipy.special import expit\n",
        "import pandas as pd\n",
        "import pickle\n",
        "from numba import jit as njit\n",
        "from functools import lru_cache\n",
        "from multiprocessing import Pool\n",
        "import os\n",
        "import time\n",
        "import random\n",
        "import ta\n",
        "from net import *\n",
        "from memory import *\n",
        "from reward import *\n",
        "import traceback\n",
        "from IPython.display import clear_output"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YkD_C2E8NaLX",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def swish(x):\n",
        "    x *= tf.nn.sigmoid(x)\n",
        "    return x\n",
        "\n",
        "def actor(x):\n",
        "  with tf.variable_scope(\"actor\", reuse=False):\n",
        "    with tf.variable_scope(\"main\", reuse=False):\n",
        "      x1 = tf.keras.layers.Conv1D(128,8,1,padding=\"causal\",activation=tf.nn.relu)(x)\n",
        "      x2 = tf.keras.layers.Conv1D(128,4,1,padding=\"causal\",activation=tf.nn.relu)(x)\n",
        "      x3 = tf.keras.layers.Conv1D(128,2,1,padding=\"causal\",activation=tf.nn.relu)(x)\n",
        "      x = tf.keras.layers.Concatenate()([x1,x2,x3])\n",
        "      x = tf.keras.layers.Conv1D(128,3,1,padding=\"valid\",activation=tf.nn.relu)(x)\n",
        "      f = tf.keras.layers.Flatten()(x)\n",
        "    with tf.variable_scope(\"sub\", reuse=False):\n",
        "      out = tf.keras.layers.Dense(128,tf.nn.relu)(f)\n",
        "      out = tf.keras.layers.Dense(3,tf.tanh)(out)\n",
        "    with tf.variable_scope(\"main2\", reuse=False):\n",
        "      # x = tf.keras.layers.Dense(128,tf.nn.relu)(f)\n",
        "      x = tf.keras.layers.Concatenate()([out,f])\n",
        "      x = tf.keras.layers.Dense(128,tf.nn.relu)(x)\n",
        "      x = tf.keras.layers.Dense(3)(x)\n",
        "      X = tf.keras.layers.Maximum()([x[:,0],x[:,1],x[:,2]])\n",
        "  return X,x,out"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mnaWdzaJY8tS",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class Agent:\n",
        "    def __init__(self,path,window_siz):\n",
        "      self.path = path\n",
        "      self.window_size = window_size\n",
        "      self.STEP_SIZE = 24 * 5\n",
        "      self.preproc()\n",
        "      self.state_size = (None,self.window_size,self.df.shape[-1])\n",
        "      print(self.state_size)\n",
        "      self.memory = Memory(5000)\n",
        "      self.rewards = reward2\n",
        "      self.sess = tf.Session()\n",
        "\n",
        "      with tf.variable_scope(\"input\"):\n",
        "        self.state = tf.placeholder(tf.float32, self.state_size)\n",
        "        self.new_state = tf.placeholder(tf.float32, self.state_size)\n",
        "        self.action = tf.placeholder(tf.float32,(None,3))\n",
        "        self.reward = tf.placeholder(tf.float32,(None,3))\n",
        "\n",
        "      with tf.variable_scope(\"model\", reuse=False):\n",
        "        self.policy,self.q,self.out = actor(self.state)\n",
        "      \n",
        "      with tf.variable_scope(\"target\", reuse=False):\n",
        "        self.target_q,_,_ = actor(self.new_state)\n",
        "\n",
        "      with tf.variable_scope(\"loss\"):\n",
        "        self.loss = loss = (0.5 * tf.reduce_mean((self.reward - self.q) ** 2))\n",
        "        self.ploss = policy_loss = -tf.reduce_mean(self.policy)\n",
        "        self.absolute_errors = tf.abs(self.reward - self.q)\n",
        "\n",
        "      self.policy_sub_opt = tf.train.AdamOptimizer(1e-5).minimize(policy_loss, var_list=get_vars('model/actor/sub'))\n",
        "      # gvs = self.policy_sub_opt.compute_gradients(policy_loss,var_list=get_vars('model/actor/sub'))\n",
        "      # capped_gvs = [(tf.clip_by_value(grad, -0.01, 0.01), var) for grad, var in gvs]\n",
        "      # self.policy_sub_opt = self.policy_sub_opt.apply_gradients(capped_gvs)\n",
        "      self.policy_main_opt = tf.train.AdamOptimizer(1e-3).minimize(loss, var_list=get_vars('model/actor/main')+get_vars('model/actor/main2'))\n",
        "\n",
        "      self.target_update = tf.group([tf.assign(v_targ, 0.555*v_targ + (1-0.555)*v_main)\n",
        "                                for v_main, v_targ in zip(get_vars('model'), get_vars('target'))])\n",
        "\n",
        "      target_init = tf.group([tf.assign(v_targ, v_main)\n",
        "                              for v_main, v_targ in zip(get_vars('model'), get_vars('target'))])\n",
        "\n",
        "      self.sess.run(tf.global_variables_initializer())\n",
        "      # self.sess.run(target_init)\n",
        "    def preproc(self):\n",
        "        self.dat = df = pd.read_csv(self.path)\n",
        "        s = np.asanyarray(ta.stoch(df[\"High\"],df[\"Low\"],df[\"Close\"],14)).reshape((-1, 1)) - np.asanyarray(ta.stoch_signal(df[\"High\"],df[\"Low\"],df[\"Close\"],14)).reshape((-1, 1))\n",
        "        x = np.asanyarray(ta.daily_return(df[\"Close\"])).reshape((-1,1))\n",
        "        m = np.asanyarray(ta.macd_diff(df[\"Close\"])).reshape((-1,1))\n",
        "        cross1 = np.asanyarray(ta.ema(self.dat[\"Close\"],12)).reshape((-1, 1)) - np.asanyarray(ta.ema(self.dat[\"Close\"],5)).reshape((-1, 1))\n",
        "        trend = np.asanyarray(df[[\"Close\"]]) - np.asanyarray(ta.ema(self.dat[\"Close\"],50)).reshape((-1, 1))\n",
        "        # x = s\n",
        "        x = np.concatenate([s,x,cross1], 1)\n",
        "        y = np.asanyarray(self.dat[[\"Open\"]])\n",
        "\n",
        "        gen = tf.keras.preprocessing.sequence.TimeseriesGenerator(x, y, self.window_size)\n",
        "        self.x = []\n",
        "        self.y = []\n",
        "        for i in gen:\n",
        "            self.x.extend(i[0].tolist())\n",
        "            self.y.extend(i[1].tolist())\n",
        "        self.x = np.asanyarray(self.x)[100:] #.reshape((-1, self.window_size, x.shape[-1]))\n",
        "        self.y = np.asanyarray(self.y)[100:]\n",
        "\n",
        "        self.df = self.x[-self.STEP_SIZE:]\n",
        "        self.trend = self.y[-self.STEP_SIZE:]\n",
        "\n",
        "    def _construct_memories_and_train(self,i, replay=None):\n",
        "      try:\n",
        "          tree_idx, replay = self.memory.sample(128)\n",
        "      except:\n",
        "          self.memory = Memory(5000)\n",
        "\n",
        "      states = np.array([a[0][0] for a in replay])\n",
        "      new_states = np.array([a[0][3] for a in replay])\n",
        "      actions = np.array([a[0][1] for a in replay]).reshape((-1, 3))\n",
        "      rewards = np.array([a[0][2] for a in replay]).reshape((-1, 1))\n",
        "\n",
        "      target_q = self.sess.run(self.target_q, feed_dict={self.new_state:new_states}).reshape((-1,1))\n",
        "      q = self.sess.run(self.q,feed_dict={self.state:states}).reshape((-1,3))\n",
        "\n",
        "      for I in range(128):\n",
        "        q[I,np.argmax(actions[I])] = rewards[I] + 0. * target_q[I]\n",
        "      # print(q)\n",
        "\n",
        "      step_ops = [self.absolute_errors,self.policy_main_opt]\n",
        "      for _ in range(1):\n",
        "        absolute_errors,_ = self.sess.run(step_ops, feed_dict={self.state: states, self.new_state: new_states, self.reward: q})\n",
        "      if i > 10:\n",
        "        if (i+1) % 2 == 0:\n",
        "              loss,_ = self.sess.run([self.loss,self.policy_sub_opt], feed_dict={self.state: states,self.reward: q})\n",
        "              # print(loss)\n",
        "          # print([rewards,loss])\n",
        "      self.sess.run(self.target_update)\n",
        "      ae = []\n",
        "      for i in absolute_errors:\n",
        "        ae.append(np.mean(i))\n",
        "      ae = np.array(ae)\n",
        "      self.memory.batch_update(tree_idx, ae)\n",
        "\n",
        "    def _select_action(self, state, i):\n",
        "        # self.policy_out\n",
        "        prediction,q = self.sess.run([self.out,self.q], feed_dict={self.state: [state]})\n",
        "        prediction,q = prediction[0],q[0]\n",
        "        # q = np.exp(np.clip(q, -25, 25))\n",
        "        # q = q / np.sum(q)\n",
        "\n",
        "        noise = 0. if (i + 1) % 5 == 0 else 0.1\n",
        "        if noise != 0.:\n",
        "            # prediction = np.exp(np.clip(prediction, -25, 25))\n",
        "            # prediction = prediction / np.sum(prediction)\n",
        "            prediction += noise * np.random.randn(3)\n",
        "            # prediction = np.clip(prediction, -1, 1)\n",
        "        q *= prediction\n",
        "        prediction = q\n",
        "        action = np.argmax(prediction)\n",
        "        self.pred = prediction\n",
        "\n",
        "        return action\n",
        "\n",
        "    def prob(self,history):\n",
        "        prob = np.asanyarray(history)\n",
        "        a = np.mean(prob == 0)\n",
        "        b = np.mean(prob == 1)\n",
        "        c = 1 - (a + b)\n",
        "        prob = [a,b,c]\n",
        "        return prob\n",
        "\n",
        "    def discount_rewards(self, r, running_add):\n",
        "        running_add = running_add * 0.99 + r\n",
        "        return running_add\n",
        "        \n",
        "    def nstep(self,r):\n",
        "        running_add = 0.0\n",
        "        for t in range(len(r)):\n",
        "            running_add += 0.99 * r[t]\n",
        "\n",
        "        return running_add\n",
        "\n",
        "    def run(self, spread=10, pip_cost=1000, los_cut=300, day_pip=24, n=10,step=100):\n",
        "        spread = spread / pip_cost\n",
        "        self.rand = np.random.RandomState()\n",
        "        lc = los_cut / pip_cost\n",
        "        for i in range(100000):\n",
        "            # if (i - 1) % step == 0:\n",
        "            # h = self.rand.randint(self.x.shape[0]-(self.STEP_SIZE+1))\n",
        "            # self.df = self.x[h:h+self.STEP_SIZE]\n",
        "            # self.trend = self.y[h:h+self.STEP_SIZE]\n",
        "            done = 0.0\n",
        "            position = 3\n",
        "            pip = []\n",
        "            provisional_pip = []\n",
        "            running_add = 0.0\n",
        "            total_pip = 0.0\n",
        "            old_reword = 0.0\n",
        "            states = []\n",
        "            h_a = []\n",
        "            h_r = []\n",
        "            h_p = []\n",
        "            old = np.asanyarray(0)\n",
        "            self.history = []\n",
        "            for t in  range(0, len(self.trend)-1):\n",
        "                action = self._select_action(self.df[t],i)\n",
        "                h_a.append(self.pred)\n",
        "                self.history.append(action)\n",
        "                \n",
        "                states,provisional_pip,position,total_pip = self.rewards(self.trend[t],pip,provisional_pip,\n",
        "                                    action,position,states,pip_cost,spread,total_pip,lc=los_cut/2/pip_cost)\n",
        "                h_p.append(position)\n",
        "                reward =  (total_pip - old_reword) * 100\n",
        "                old_reword = total_pip\n",
        "                h_r.append(reward)\n",
        "\n",
        "                # running_add = self.discount_rewards(reward,running_add)\n",
        "                # r_d = int(running_add * (pip_cost / 100)) * 100\n",
        "                # r_d = int(np.mean(np.array(provisional_pip) > 0)*100) if len(provisional_pip) != 0 else 0.\n",
        "                # if t == len(self.trend)-1:\n",
        "                #     done = 1.0\n",
        "                # exp = self.df[t], h_a[t], r_d, self.df[t+1], done\n",
        "                # self.memory.store(exp)\n",
        "            for t in range(0, len(self.trend)-1):\n",
        "                tau = t - n + 1\n",
        "                if tau >= 0:\n",
        "                    rewards = self.nstep(h_r[tau+1:tau+n])\n",
        "                    exp = self.df[t], h_a[t], int(rewards), self.df[t+1], done\n",
        "                    self.memory.store(exp)\n",
        "\n",
        "            try:\n",
        "              self._construct_memories_and_train(i)\n",
        "            except:\n",
        "              traceback.print_exc()\n",
        "\n",
        "            if i % 1000 == 0:\n",
        "              clear_output()\n",
        "\n",
        "            if (i + 1) % 5 == 0:\n",
        "                # loss = np.mean(ae)\n",
        "                self.pip = np.asanyarray(provisional_pip) * pip_cost\n",
        "                self.pip = [p if p >= -los_cut else -los_cut for p in self.pip]\n",
        "                self.total_pip = np.sum(self.pip)\n",
        "                mean_pip = self.total_pip / (t + 1)\n",
        "                trade_accuracy = np.mean(np.asanyarray(self.pip) > 0)\n",
        "                self.trade = trade_accuracy\n",
        "                mean_pip *= day_pip\n",
        "                prob = self.prob(self.history)\n",
        "                position_prob = self.prob(h_p)\n",
        "\n",
        "                # print(\"loss =\", loss)\n",
        "                # print(\"\")\n",
        "                print('action probability = ', prob)\n",
        "                print(\"buy = \", position_prob[1], \" sell = \", position_prob[-1])\n",
        "                print('trade accuracy = ', trade_accuracy)\n",
        "                print('epoch: %d, total rewards: %f, mean rewards: %f' % (i + 1, float(self.total_pip), float(mean_pip)))\n",
        "                print(\"\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hFioYWzBenRt",
        "colab_type": "code",
        "outputId": "03d8d294-c7c6-4d40-91fd-4b4d19c004c4",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "window_size = 30\n",
        "path = \"audpred60.csv\"\n",
        "agent = Agent(path,window_size)\n",
        "\n",
        "agent.run()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "action probability =  [0.40336134453781514, 0.42857142857142855, 0.16806722689075637]\n",
            "buy =  0.44537815126050423  sell =  0.5546218487394958\n",
            "trade accuracy =  0.46464646464646464\n",
            "epoch: 3005, total rewards: 4080.000000, mean rewards: 822.857143\n",
            "\n",
            "action probability =  [0.35294117647058826, 0.4789915966386555, 0.16806722689075626]\n",
            "buy =  0.42016806722689076  sell =  0.5798319327731092\n",
            "trade accuracy =  0.47474747474747475\n",
            "epoch: 3010, total rewards: 2915.000000, mean rewards: 587.899160\n",
            "\n",
            "action probability =  [0.3697478991596639, 0.48739495798319327, 0.1428571428571428]\n",
            "buy =  0.4117647058823529  sell =  0.5882352941176471\n",
            "trade accuracy =  0.5294117647058824\n",
            "epoch: 3015, total rewards: 4514.000000, mean rewards: 910.386555\n",
            "\n",
            "action probability =  [0.44537815126050423, 0.40336134453781514, 0.15126050420168058]\n",
            "buy =  0.48739495798319327  sell =  0.5126050420168067\n",
            "trade accuracy =  0.504950495049505\n",
            "epoch: 3020, total rewards: 5608.000000, mean rewards: 1131.025210\n",
            "\n",
            "action probability =  [0.42857142857142855, 0.47058823529411764, 0.10084033613445387]\n",
            "buy =  0.453781512605042  sell =  0.546218487394958\n",
            "trade accuracy =  0.5327102803738317\n",
            "epoch: 3025, total rewards: 6198.000000, mean rewards: 1250.016807\n",
            "\n",
            "action probability =  [0.36134453781512604, 0.4789915966386555, 0.15966386554621848]\n",
            "buy =  0.42857142857142855  sell =  0.5714285714285714\n",
            "trade accuracy =  0.51\n",
            "epoch: 3030, total rewards: 5622.000000, mean rewards: 1133.848739\n",
            "\n",
            "action probability =  [0.29411764705882354, 0.46218487394957986, 0.24369747899159666]\n",
            "buy =  0.4117647058823529  sell =  0.5882352941176471\n",
            "trade accuracy =  0.4888888888888889\n",
            "epoch: 3035, total rewards: 4779.000000, mean rewards: 963.831933\n",
            "\n",
            "action probability =  [0.25210084033613445, 0.5042016806722689, 0.24369747899159666]\n",
            "buy =  0.3445378151260504  sell =  0.6554621848739496\n",
            "trade accuracy =  0.4777777777777778\n",
            "epoch: 3040, total rewards: 4691.000000, mean rewards: 946.084034\n",
            "\n",
            "action probability =  [0.33613445378151263, 0.453781512605042, 0.2100840336134453]\n",
            "buy =  0.453781512605042  sell =  0.546218487394958\n",
            "trade accuracy =  0.4148936170212766\n",
            "epoch: 3045, total rewards: 2839.000000, mean rewards: 572.571429\n",
            "\n",
            "action probability =  [0.2773109243697479, 0.35294117647058826, 0.3697478991596639]\n",
            "buy =  0.5546218487394958  sell =  0.4453781512605042\n",
            "trade accuracy =  0.5333333333333333\n",
            "epoch: 3050, total rewards: 13583.000000, mean rewards: 2739.428571\n",
            "\n",
            "action probability =  [0.23529411764705882, 0.44537815126050423, 0.31932773109243695]\n",
            "buy =  0.42016806722689076  sell =  0.5798319327731092\n",
            "trade accuracy =  0.5555555555555556\n",
            "epoch: 3055, total rewards: 10900.000000, mean rewards: 2198.319328\n",
            "\n",
            "action probability =  [0.29411764705882354, 0.33613445378151263, 0.3697478991596639]\n",
            "buy =  0.5462184873949579  sell =  0.45378151260504207\n",
            "trade accuracy =  0.6666666666666666\n",
            "epoch: 3060, total rewards: 17065.000000, mean rewards: 3441.680672\n",
            "\n",
            "action probability =  [0.2689075630252101, 0.4117647058823529, 0.31932773109243695]\n",
            "buy =  0.4369747899159664  sell =  0.5630252100840336\n",
            "trade accuracy =  0.6172839506172839\n",
            "epoch: 3065, total rewards: 10361.000000, mean rewards: 2089.613445\n",
            "\n",
            "action probability =  [0.3697478991596639, 0.33613445378151263, 0.2941176470588235]\n",
            "buy =  0.5882352941176471  sell =  0.4117647058823529\n",
            "trade accuracy =  0.5238095238095238\n",
            "epoch: 3070, total rewards: 5811.000000, mean rewards: 1171.966387\n",
            "\n",
            "action probability =  [0.25210084033613445, 0.4957983193277311, 0.25210084033613445]\n",
            "buy =  0.3697478991596639  sell =  0.6302521008403361\n",
            "trade accuracy =  0.5227272727272727\n",
            "epoch: 3075, total rewards: 6682.000000, mean rewards: 1347.630252\n",
            "\n",
            "action probability =  [0.3025210084033613, 0.42016806722689076, 0.2773109243697479]\n",
            "buy =  0.42016806722689076  sell =  0.5798319327731092\n",
            "trade accuracy =  0.5833333333333334\n",
            "epoch: 3080, total rewards: 7288.000000, mean rewards: 1469.848739\n",
            "\n",
            "action probability =  [0.2605042016806723, 0.44537815126050423, 0.2941176470588235]\n",
            "buy =  0.3697478991596639  sell =  0.6302521008403361\n",
            "trade accuracy =  0.5487804878048781\n",
            "epoch: 3085, total rewards: 5406.000000, mean rewards: 1090.285714\n",
            "\n",
            "action probability =  [0.2773109243697479, 0.47058823529411764, 0.25210084033613445]\n",
            "buy =  0.36134453781512604  sell =  0.6386554621848739\n",
            "trade accuracy =  0.5402298850574713\n",
            "epoch: 3090, total rewards: 4653.000000, mean rewards: 938.420168\n",
            "\n",
            "action probability =  [0.3277310924369748, 0.4117647058823529, 0.26050420168067223]\n",
            "buy =  0.42857142857142855  sell =  0.5714285714285714\n",
            "trade accuracy =  0.4069767441860465\n",
            "epoch: 3095, total rewards: 499.000000, mean rewards: 100.638655\n",
            "\n",
            "action probability =  [0.31092436974789917, 0.5126050420168067, 0.17647058823529416]\n",
            "buy =  0.3697478991596639  sell =  0.6302521008403361\n",
            "trade accuracy =  0.4375\n",
            "epoch: 3100, total rewards: 1258.000000, mean rewards: 253.714286\n",
            "\n",
            "action probability =  [0.3697478991596639, 0.453781512605042, 0.17647058823529416]\n",
            "buy =  0.42857142857142855  sell =  0.5714285714285714\n",
            "trade accuracy =  0.4375\n",
            "epoch: 3105, total rewards: 1124.000000, mean rewards: 226.689076\n",
            "\n",
            "action probability =  [0.40336134453781514, 0.4369747899159664, 0.15966386554621848]\n",
            "buy =  0.44537815126050423  sell =  0.5546218487394958\n",
            "trade accuracy =  0.3877551020408163\n",
            "epoch: 3110, total rewards: -622.000000, mean rewards: -125.445378\n",
            "\n",
            "action probability =  [0.40336134453781514, 0.3865546218487395, 0.2100840336134453]\n",
            "buy =  0.4789915966386555  sell =  0.5210084033613445\n",
            "trade accuracy =  0.43478260869565216\n",
            "epoch: 3115, total rewards: 3603.000000, mean rewards: 726.655462\n",
            "\n",
            "action probability =  [0.4117647058823529, 0.3697478991596639, 0.2184873949579832]\n",
            "buy =  0.5210084033613446  sell =  0.4789915966386554\n",
            "trade accuracy =  0.46153846153846156\n",
            "epoch: 3120, total rewards: 5012.000000, mean rewards: 1010.823529\n",
            "\n",
            "action probability =  [0.4117647058823529, 0.3949579831932773, 0.19327731092436973]\n",
            "buy =  0.4789915966386555  sell =  0.5210084033613445\n",
            "trade accuracy =  0.425531914893617\n",
            "epoch: 3125, total rewards: 2660.000000, mean rewards: 536.470588\n",
            "\n",
            "action probability =  [0.3949579831932773, 0.42016806722689076, 0.18487394957983194]\n",
            "buy =  0.44537815126050423  sell =  0.5546218487394958\n",
            "trade accuracy =  0.45263157894736844\n",
            "epoch: 3130, total rewards: 3021.000000, mean rewards: 609.277311\n",
            "\n",
            "action probability =  [0.4117647058823529, 0.31932773109243695, 0.26890756302521013]\n",
            "buy =  0.48739495798319327  sell =  0.5126050420168067\n",
            "trade accuracy =  0.4578313253012048\n",
            "epoch: 3135, total rewards: 3112.000000, mean rewards: 627.630252\n",
            "\n",
            "action probability =  [0.37815126050420167, 0.33613445378151263, 0.2857142857142857]\n",
            "buy =  0.5126050420168067  sell =  0.4873949579831933\n",
            "trade accuracy =  0.46987951807228917\n",
            "epoch: 3140, total rewards: 5118.000000, mean rewards: 1032.201681\n",
            "\n",
            "action probability =  [0.37815126050420167, 0.37815126050420167, 0.24369747899159666]\n",
            "buy =  0.47058823529411764  sell =  0.5294117647058824\n",
            "trade accuracy =  0.4772727272727273\n",
            "epoch: 3145, total rewards: 4376.000000, mean rewards: 882.554622\n",
            "\n",
            "action probability =  [0.4369747899159664, 0.35294117647058826, 0.2100840336134453]\n",
            "buy =  0.5378151260504201  sell =  0.46218487394957986\n",
            "trade accuracy =  0.5164835164835165\n",
            "epoch: 3150, total rewards: 5024.000000, mean rewards: 1013.243697\n",
            "\n",
            "action probability =  [0.35294117647058826, 0.42857142857142855, 0.2184873949579832]\n",
            "buy =  0.453781512605042  sell =  0.546218487394958\n",
            "trade accuracy =  0.5222222222222223\n",
            "epoch: 3155, total rewards: 6089.000000, mean rewards: 1228.033613\n",
            "\n",
            "action probability =  [0.3697478991596639, 0.36134453781512604, 0.26890756302521]\n",
            "buy =  0.48739495798319327  sell =  0.5126050420168067\n",
            "trade accuracy =  0.47619047619047616\n",
            "epoch: 3160, total rewards: 5212.000000, mean rewards: 1051.159664\n",
            "\n",
            "action probability =  [0.35294117647058826, 0.40336134453781514, 0.24369747899159666]\n",
            "buy =  0.453781512605042  sell =  0.546218487394958\n",
            "trade accuracy =  0.41379310344827586\n",
            "epoch: 3165, total rewards: 1462.000000, mean rewards: 294.857143\n",
            "\n",
            "action probability =  [0.2605042016806723, 0.5042016806722689, 0.23529411764705888]\n",
            "buy =  0.37815126050420167  sell =  0.6218487394957983\n",
            "trade accuracy =  0.4318181818181818\n",
            "epoch: 3170, total rewards: 1282.000000, mean rewards: 258.554622\n",
            "\n",
            "action probability =  [0.37815126050420167, 0.37815126050420167, 0.24369747899159666]\n",
            "buy =  0.46218487394957986  sell =  0.5378151260504201\n",
            "trade accuracy =  0.41379310344827586\n",
            "epoch: 3175, total rewards: 3031.000000, mean rewards: 611.294118\n",
            "\n",
            "action probability =  [0.2184873949579832, 0.6470588235294118, 0.134453781512605]\n",
            "buy =  0.25210084033613445  sell =  0.7478991596638656\n",
            "trade accuracy =  0.42\n",
            "epoch: 3180, total rewards: -254.000000, mean rewards: -51.226891\n",
            "\n",
            "action probability =  [0.31092436974789917, 0.5126050420168067, 0.17647058823529416]\n",
            "buy =  0.3697478991596639  sell =  0.6302521008403361\n",
            "trade accuracy =  0.4\n",
            "epoch: 3185, total rewards: -841.000000, mean rewards: -169.613445\n",
            "\n",
            "action probability =  [0.15126050420168066, 0.6638655462184874, 0.18487394957983194]\n",
            "buy =  0.2184873949579832  sell =  0.7815126050420168\n",
            "trade accuracy =  0.425531914893617\n",
            "epoch: 3190, total rewards: 1632.000000, mean rewards: 329.142857\n",
            "\n",
            "action probability =  [0.17647058823529413, 0.7310924369747899, 0.09243697478991597]\n",
            "buy =  0.2184873949579832  sell =  0.7815126050420168\n",
            "trade accuracy =  0.41904761904761906\n",
            "epoch: 3195, total rewards: 1110.000000, mean rewards: 223.865546\n",
            "\n",
            "action probability =  [0.15126050420168066, 0.7899159663865546, 0.05882352941176472]\n",
            "buy =  0.15126050420168066  sell =  0.8487394957983193\n",
            "trade accuracy =  0.44036697247706424\n",
            "epoch: 3200, total rewards: 1430.000000, mean rewards: 288.403361\n",
            "\n",
            "action probability =  [0.16806722689075632, 0.7394957983193278, 0.09243697478991586]\n",
            "buy =  0.2184873949579832  sell =  0.7815126050420168\n",
            "trade accuracy =  0.45714285714285713\n",
            "epoch: 3205, total rewards: 2522.000000, mean rewards: 508.638655\n",
            "\n",
            "action probability =  [0.2773109243697479, 0.5630252100840336, 0.15966386554621848]\n",
            "buy =  0.40336134453781514  sell =  0.5966386554621849\n",
            "trade accuracy =  0.36082474226804123\n",
            "epoch: 3210, total rewards: -653.000000, mean rewards: -131.697479\n",
            "\n",
            "action probability =  [0.2184873949579832, 0.6386554621848739, 0.1428571428571429]\n",
            "buy =  0.3445378151260504  sell =  0.6554621848739496\n",
            "trade accuracy =  0.42424242424242425\n",
            "epoch: 3215, total rewards: 905.000000, mean rewards: 182.521008\n",
            "\n",
            "action probability =  [0.16806722689075632, 0.7394957983193278, 0.09243697478991586]\n",
            "buy =  0.226890756302521  sell =  0.773109243697479\n",
            "trade accuracy =  0.44761904761904764\n",
            "epoch: 3220, total rewards: 2437.000000, mean rewards: 491.495798\n",
            "\n",
            "action probability =  [0.16806722689075632, 0.6134453781512605, 0.21848739495798308]\n",
            "buy =  0.2773109243697479  sell =  0.7226890756302521\n",
            "trade accuracy =  0.3111111111111111\n",
            "epoch: 3225, total rewards: -2755.000000, mean rewards: -555.630252\n",
            "\n",
            "action probability =  [0.24369747899159663, 0.5462184873949579, 0.2100840336134454]\n",
            "buy =  0.42016806722689076  sell =  0.5798319327731092\n",
            "trade accuracy =  0.3626373626373626\n",
            "epoch: 3230, total rewards: -3153.000000, mean rewards: -635.899160\n",
            "\n",
            "action probability =  [0.14285714285714285, 0.6218487394957983, 0.23529411764705888]\n",
            "buy =  0.25210084033613445  sell =  0.7478991596638656\n",
            "trade accuracy =  0.3977272727272727\n",
            "epoch: 3235, total rewards: 1394.000000, mean rewards: 281.142857\n",
            "\n",
            "action probability =  [0.25210084033613445, 0.42857142857142855, 0.31932773109243695]\n",
            "buy =  0.48739495798319327  sell =  0.5126050420168067\n",
            "trade accuracy =  0.3333333333333333\n",
            "epoch: 3240, total rewards: -3653.000000, mean rewards: -736.739496\n",
            "\n",
            "action probability =  [0.25210084033613445, 0.5126050420168067, 0.23529411764705888]\n",
            "buy =  0.46218487394957986  sell =  0.5378151260504201\n",
            "trade accuracy =  0.29545454545454547\n",
            "epoch: 3245, total rewards: -5982.000000, mean rewards: -1206.453782\n",
            "\n",
            "action probability =  [0.29411764705882354, 0.46218487394957986, 0.24369747899159666]\n",
            "buy =  0.4957983193277311  sell =  0.5042016806722689\n",
            "trade accuracy =  0.3448275862068966\n",
            "epoch: 3250, total rewards: -3669.000000, mean rewards: -739.966387\n",
            "\n",
            "action probability =  [0.25210084033613445, 0.5210084033613446, 0.22689075630252098]\n",
            "buy =  0.453781512605042  sell =  0.546218487394958\n",
            "trade accuracy =  0.33707865168539325\n",
            "epoch: 3255, total rewards: -4605.000000, mean rewards: -928.739496\n",
            "\n",
            "action probability =  [0.25210084033613445, 0.47058823529411764, 0.2773109243697479]\n",
            "buy =  0.4789915966386555  sell =  0.5210084033613445\n",
            "trade accuracy =  0.27710843373493976\n",
            "epoch: 3260, total rewards: -4979.000000, mean rewards: -1004.168067\n",
            "\n",
            "action probability =  [0.31932773109243695, 0.4117647058823529, 0.26890756302521013]\n",
            "buy =  0.5042016806722689  sell =  0.4957983193277311\n",
            "trade accuracy =  0.30952380952380953\n",
            "epoch: 3265, total rewards: -3991.000000, mean rewards: -804.907563\n",
            "\n",
            "action probability =  [0.3025210084033613, 0.46218487394957986, 0.23529411764705888]\n",
            "buy =  0.4789915966386555  sell =  0.5210084033613445\n",
            "trade accuracy =  0.2840909090909091\n",
            "epoch: 3270, total rewards: -5724.000000, mean rewards: -1154.420168\n",
            "\n",
            "action probability =  [0.29411764705882354, 0.42857142857142855, 0.2773109243697479]\n",
            "buy =  0.4957983193277311  sell =  0.5042016806722689\n",
            "trade accuracy =  0.2891566265060241\n",
            "epoch: 3275, total rewards: -4472.000000, mean rewards: -901.915966\n",
            "\n",
            "action probability =  [0.2689075630252101, 0.42016806722689076, 0.31092436974789917]\n",
            "buy =  0.47058823529411764  sell =  0.5294117647058824\n",
            "trade accuracy =  0.3037974683544304\n",
            "epoch: 3280, total rewards: -2890.000000, mean rewards: -582.857143\n",
            "\n",
            "action probability =  [0.2689075630252101, 0.48739495798319327, 0.24369747899159666]\n",
            "buy =  0.44537815126050423  sell =  0.5546218487394958\n",
            "trade accuracy =  0.3218390804597701\n",
            "epoch: 3285, total rewards: -3969.000000, mean rewards: -800.470588\n",
            "\n",
            "action probability =  [0.31092436974789917, 0.3865546218487395, 0.3025210084033614]\n",
            "buy =  0.5126050420168067  sell =  0.4873949579831933\n",
            "trade accuracy =  0.325\n",
            "epoch: 3290, total rewards: -2196.000000, mean rewards: -442.890756\n",
            "\n",
            "action probability =  [0.226890756302521, 0.5042016806722689, 0.26890756302521013]\n",
            "buy =  0.4117647058823529  sell =  0.5882352941176471\n",
            "trade accuracy =  0.42857142857142855\n",
            "epoch: 3295, total rewards: 1871.000000, mean rewards: 377.344538\n",
            "\n",
            "action probability =  [0.24369747899159663, 0.4957983193277311, 0.26050420168067223]\n",
            "buy =  0.4117647058823529  sell =  0.5882352941176471\n",
            "trade accuracy =  0.4\n",
            "epoch: 3300, total rewards: 768.000000, mean rewards: 154.890756\n",
            "\n",
            "action probability =  [0.2605042016806723, 0.4789915966386555, 0.26050420168067223]\n",
            "buy =  0.44537815126050423  sell =  0.5546218487394958\n",
            "trade accuracy =  0.3764705882352941\n",
            "epoch: 3305, total rewards: -875.000000, mean rewards: -176.470588\n",
            "\n",
            "action probability =  [0.24369747899159663, 0.47058823529411764, 0.2857142857142857]\n",
            "buy =  0.37815126050420167  sell =  0.6218487394957983\n",
            "trade accuracy =  0.4146341463414634\n",
            "epoch: 3310, total rewards: 1127.000000, mean rewards: 227.294118\n",
            "\n",
            "action probability =  [0.226890756302521, 0.5126050420168067, 0.26050420168067234]\n",
            "buy =  0.36134453781512604  sell =  0.6386554621848739\n",
            "trade accuracy =  0.4117647058823529\n",
            "epoch: 3315, total rewards: 1383.000000, mean rewards: 278.924370\n",
            "\n",
            "action probability =  [0.2773109243697479, 0.47058823529411764, 0.25210084033613445]\n",
            "buy =  0.48739495798319327  sell =  0.5126050420168067\n",
            "trade accuracy =  0.36046511627906974\n",
            "epoch: 3320, total rewards: -3271.000000, mean rewards: -659.697479\n",
            "\n",
            "action probability =  [0.23529411764705882, 0.5462184873949579, 0.2184873949579833]\n",
            "buy =  0.36134453781512604  sell =  0.6386554621848739\n",
            "trade accuracy =  0.4777777777777778\n",
            "epoch: 3325, total rewards: 3113.000000, mean rewards: 627.831933\n",
            "\n",
            "action probability =  [0.24369747899159663, 0.5042016806722689, 0.25210084033613445]\n",
            "buy =  0.3865546218487395  sell =  0.6134453781512605\n",
            "trade accuracy =  0.45348837209302323\n",
            "epoch: 3330, total rewards: 891.000000, mean rewards: 179.697479\n",
            "\n",
            "action probability =  [0.2689075630252101, 0.4789915966386555, 0.25210084033613445]\n",
            "buy =  0.4117647058823529  sell =  0.5882352941176471\n",
            "trade accuracy =  0.45348837209302323\n",
            "epoch: 3335, total rewards: 665.000000, mean rewards: 134.117647\n",
            "\n",
            "action probability =  [0.226890756302521, 0.5546218487394958, 0.2184873949579832]\n",
            "buy =  0.33613445378151263  sell =  0.6638655462184874\n",
            "trade accuracy =  0.5\n",
            "epoch: 3340, total rewards: 3866.000000, mean rewards: 779.697479\n",
            "\n",
            "action probability =  [0.2184873949579832, 0.5966386554621849, 0.18487394957983194]\n",
            "buy =  0.3277310924369748  sell =  0.6722689075630253\n",
            "trade accuracy =  0.5\n",
            "epoch: 3345, total rewards: 4716.000000, mean rewards: 951.126050\n",
            "\n",
            "action probability =  [0.33613445378151263, 0.5210084033613446, 0.1428571428571428]\n",
            "buy =  0.4117647058823529  sell =  0.5882352941176471\n",
            "trade accuracy =  0.5151515151515151\n",
            "epoch: 3350, total rewards: 2843.000000, mean rewards: 573.378151\n",
            "\n",
            "action probability =  [0.3445378151260504, 0.5378151260504201, 0.11764705882352944]\n",
            "buy =  0.42016806722689076  sell =  0.5798319327731092\n",
            "trade accuracy =  0.5196078431372549\n",
            "epoch: 3355, total rewards: 3057.000000, mean rewards: 616.537815\n",
            "\n",
            "action probability =  [0.24369747899159663, 0.6050420168067226, 0.1512605042016807]\n",
            "buy =  0.3445378151260504  sell =  0.6554621848739496\n",
            "trade accuracy =  0.47959183673469385\n",
            "epoch: 3360, total rewards: 3111.000000, mean rewards: 627.428571\n",
            "\n",
            "action probability =  [0.25210084033613445, 0.5630252100840336, 0.18487394957983194]\n",
            "buy =  0.33613445378151263  sell =  0.6638655462184874\n",
            "trade accuracy =  0.4574468085106383\n",
            "epoch: 3365, total rewards: 1878.000000, mean rewards: 378.756303\n",
            "\n",
            "action probability =  [0.2773109243697479, 0.5714285714285714, 0.1512605042016807]\n",
            "buy =  0.37815126050420167  sell =  0.6218487394957983\n",
            "trade accuracy =  0.47959183673469385\n",
            "epoch: 3370, total rewards: 1920.000000, mean rewards: 387.226891\n",
            "\n",
            "action probability =  [0.3025210084033613, 0.5798319327731093, 0.11764705882352944]\n",
            "buy =  0.3865546218487395  sell =  0.6134453781512605\n",
            "trade accuracy =  0.47058823529411764\n",
            "epoch: 3375, total rewards: 1335.000000, mean rewards: 269.243697\n",
            "\n",
            "action probability =  [0.29411764705882354, 0.6050420168067226, 0.10084033613445387]\n",
            "buy =  0.35294117647058826  sell =  0.6470588235294117\n",
            "trade accuracy =  0.4326923076923077\n",
            "epoch: 3380, total rewards: 1017.000000, mean rewards: 205.109244\n",
            "\n",
            "action probability =  [0.35294117647058826, 0.5546218487394958, 0.09243697478991586]\n",
            "buy =  0.4369747899159664  sell =  0.5630252100840336\n",
            "trade accuracy =  0.5047619047619047\n",
            "epoch: 3385, total rewards: 2616.000000, mean rewards: 527.596639\n",
            "\n",
            "action probability =  [0.29411764705882354, 0.5798319327731093, 0.12605042016806722]\n",
            "buy =  0.36134453781512604  sell =  0.6386554621848739\n",
            "trade accuracy =  0.5148514851485149\n",
            "epoch: 3390, total rewards: 4810.000000, mean rewards: 970.084034\n",
            "\n",
            "action probability =  [0.3445378151260504, 0.5714285714285714, 0.08403361344537819]\n",
            "buy =  0.4117647058823529  sell =  0.5882352941176471\n",
            "trade accuracy =  0.5\n",
            "epoch: 3395, total rewards: 2960.000000, mean rewards: 596.974790\n",
            "\n",
            "action probability =  [0.37815126050420167, 0.5378151260504201, 0.08403361344537819]\n",
            "buy =  0.453781512605042  sell =  0.546218487394958\n",
            "trade accuracy =  0.5377358490566038\n",
            "epoch: 3400, total rewards: 3404.000000, mean rewards: 686.521008\n",
            "\n",
            "action probability =  [0.31092436974789917, 0.6050420168067226, 0.08403361344537819]\n",
            "buy =  0.36134453781512604  sell =  0.6386554621848739\n",
            "trade accuracy =  0.5471698113207547\n",
            "epoch: 3405, total rewards: 3511.000000, mean rewards: 708.100840\n",
            "\n",
            "action probability =  [0.36134453781512604, 0.5546218487394958, 0.08403361344537807]\n",
            "buy =  0.42857142857142855  sell =  0.5714285714285714\n",
            "trade accuracy =  0.5188679245283019\n",
            "epoch: 3410, total rewards: 3085.000000, mean rewards: 622.184874\n",
            "\n",
            "action probability =  [0.40336134453781514, 0.5378151260504201, 0.05882352941176472]\n",
            "buy =  0.46218487394957986  sell =  0.5378151260504201\n",
            "trade accuracy =  0.5137614678899083\n",
            "epoch: 3415, total rewards: 2366.000000, mean rewards: 477.176471\n",
            "\n",
            "action probability =  [0.31092436974789917, 0.6134453781512605, 0.07563025210084029]\n",
            "buy =  0.36134453781512604  sell =  0.6386554621848739\n",
            "trade accuracy =  0.5981308411214953\n",
            "epoch: 3420, total rewards: 7830.000000, mean rewards: 1579.159664\n",
            "\n",
            "action probability =  [0.42016806722689076, 0.4789915966386555, 0.10084033613445376]\n",
            "buy =  0.5042016806722689  sell =  0.4957983193277311\n",
            "trade accuracy =  0.5480769230769231\n",
            "epoch: 3425, total rewards: 4904.000000, mean rewards: 989.042017\n",
            "\n",
            "action probability =  [0.40336134453781514, 0.4789915966386555, 0.11764705882352944]\n",
            "buy =  0.4957983193277311  sell =  0.5042016806722689\n",
            "trade accuracy =  0.5588235294117647\n",
            "epoch: 3430, total rewards: 4119.000000, mean rewards: 830.722689\n",
            "\n",
            "action probability =  [0.31932773109243695, 0.5966386554621849, 0.08403361344537819]\n",
            "buy =  0.3697478991596639  sell =  0.6302521008403361\n",
            "trade accuracy =  0.6037735849056604\n",
            "epoch: 3435, total rewards: 8703.000000, mean rewards: 1755.226891\n",
            "\n",
            "action probability =  [0.3865546218487395, 0.5294117647058824, 0.08403361344537807]\n",
            "buy =  0.44537815126050423  sell =  0.5546218487394958\n",
            "trade accuracy =  0.5943396226415094\n",
            "epoch: 3440, total rewards: 5828.000000, mean rewards: 1175.394958\n",
            "\n",
            "action probability =  [0.3277310924369748, 0.5882352941176471, 0.08403361344537807]\n",
            "buy =  0.37815126050420167  sell =  0.6218487394957983\n",
            "trade accuracy =  0.6132075471698113\n",
            "epoch: 3445, total rewards: 7379.000000, mean rewards: 1488.201681\n",
            "\n",
            "action probability =  [0.3025210084033613, 0.6134453781512605, 0.08403361344537807]\n",
            "buy =  0.35294117647058826  sell =  0.6470588235294117\n",
            "trade accuracy =  0.5943396226415094\n",
            "epoch: 3450, total rewards: 6325.000000, mean rewards: 1275.630252\n",
            "\n",
            "action probability =  [0.3865546218487395, 0.5462184873949579, 0.0672268907563025]\n",
            "buy =  0.4369747899159664  sell =  0.5630252100840336\n",
            "trade accuracy =  0.5555555555555556\n",
            "epoch: 3455, total rewards: 4982.000000, mean rewards: 1004.773109\n",
            "\n",
            "action probability =  [0.4369747899159664, 0.5042016806722689, 0.05882352941176472]\n",
            "buy =  0.4957983193277311  sell =  0.5042016806722689\n",
            "trade accuracy =  0.5412844036697247\n",
            "epoch: 3460, total rewards: 4094.000000, mean rewards: 825.680672\n",
            "\n",
            "action probability =  [0.3445378151260504, 0.5882352941176471, 0.0672268907563025]\n",
            "buy =  0.40336134453781514  sell =  0.5966386554621849\n",
            "trade accuracy =  0.5833333333333334\n",
            "epoch: 3465, total rewards: 5908.000000, mean rewards: 1191.529412\n",
            "\n",
            "action probability =  [0.4789915966386555, 0.47058823529411764, 0.050420168067226934]\n",
            "buy =  0.5294117647058824  sell =  0.47058823529411764\n",
            "trade accuracy =  0.5181818181818182\n",
            "epoch: 3470, total rewards: 4574.000000, mean rewards: 922.487395\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LYepHLr81cCD",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "(agent.pred)"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}