{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Evolution Strategies.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/komo135/tradingrl/blob/master/Evolution_Strategies.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DXpyhGyC4Cpk",
        "colab_type": "code",
        "outputId": "c0295ea0-4388-44df-e544-95fa5c6b517b",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 54
        }
      },
      "source": [
        "# Google ドライブをマウントするには、このセルを実行してください。\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "%cd drive/My Drive"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n",
            "/content/drive/My Drive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wuQVZCJv4FBu",
        "colab_type": "code",
        "outputId": "de440bf4-0988-46e5-cdd3-f1affd79782c",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 181
        }
      },
      "source": [
        "!pip install ta"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: ta in /usr/local/lib/python3.6/dist-packages (0.4.5)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (from ta) (1.16.5)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.6/dist-packages (from ta) (0.24.2)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.6/dist-packages (from ta) (0.21.3)\n",
            "Requirement already satisfied: pytz>=2011k in /usr/local/lib/python3.6/dist-packages (from pandas->ta) (2018.9)\n",
            "Requirement already satisfied: python-dateutil>=2.5.0 in /usr/local/lib/python3.6/dist-packages (from pandas->ta) (2.5.3)\n",
            "Requirement already satisfied: joblib>=0.11 in /usr/local/lib/python3.6/dist-packages (from scikit-learn->ta) (0.13.2)\n",
            "Requirement already satisfied: scipy>=0.17.0 in /usr/local/lib/python3.6/dist-packages (from scikit-learn->ta) (1.3.1)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.6/dist-packages (from python-dateutil>=2.5.0->pandas->ta) (1.12.0)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SecmHJge38IS",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import pickle\n",
        "from numba import jit as njit\n",
        "from functools import lru_cache\n",
        "import time\n",
        "import random\n",
        "import ta\n",
        "import tensorflow as tf\n",
        "from net import *\n",
        "from memory import *\n",
        "from reward import *"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rQOIhf5N4Ie1",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class Deep_Evolution_Strategy:\n",
        "\n",
        "    inputs = None\n",
        "\n",
        "    def __init__(\n",
        "        self, weights, reward_function,test_function, population_size, sigma, learning_rate\n",
        "    ):\n",
        "        self.weights = weights\n",
        "        self.reward_function = reward_function\n",
        "        self.test_function = test_function\n",
        "        self.population_size = population_size\n",
        "        self.sigma = sigma\n",
        "        self.learning_rate = learning_rate\n",
        "        \n",
        "    @njit()\n",
        "    def _get_weight_from_population(self, weights, population):\n",
        "        weights_population = []\n",
        "        for index, i in enumerate(population):\n",
        "            jittered = self.sigma * i\n",
        "            weights_population.append(weights[index] + jittered)\n",
        "        return weights_population\n",
        "\n",
        "    def get_weights(self):\n",
        "        return self.weights\n",
        "      \n",
        "    @lru_cache(1024)\n",
        "    def train(self, epoch = 100, print_every = 1):\n",
        "        lasttime = time.time()\n",
        "        for i in range(epoch):\n",
        "            population = []\n",
        "            rewards = np.zeros(self.population_size)\n",
        "            for k in range(self.population_size):\n",
        "                x = []\n",
        "                for w in self.weights:\n",
        "                    x.append(np.random.randn(*w.shape))\n",
        "                population.append(x)\n",
        "            change = True\n",
        "            for k in range(self.population_size):\n",
        "                weights_population = self._get_weight_from_population(\n",
        "                    self.weights, population[k]\n",
        "                )\n",
        "                rewards[k] = self.reward_function(weights_population, change)\n",
        "                change = False\n",
        "            rewards = (rewards - np.mean(rewards)) / (np.std(rewards) + 1e-5)\n",
        "            for index, w in enumerate(self.weights):\n",
        "                A = np.array([p[index] for p in population])\n",
        "                self.weights[index] = (\n",
        "                    w\n",
        "                    + self.learning_rate\n",
        "                    / (self.population_size * self.sigma)\n",
        "                    * np.dot(A.T, rewards).T\n",
        "                )\n",
        "            if (i + 1) % print_every == 0:\n",
        "                self.test_function(self.weights,i)\n",
        "                f = open('weights.txt', 'wb')\n",
        "                weights = self.get_weights()\n",
        "                pickle.dump(weights, f)\n",
        "                \n",
        "        print('time taken to train:', time.time() - lasttime, 'seconds')\n",
        "\n",
        "\n",
        "class Model:\n",
        "    def __init__(self, input_size, layer_size, output_size=3,restore=False):\n",
        "      if restore:\n",
        "        f = open(\"./weights.txt\",\"rb\")\n",
        "        self.weights = pickle.load(f)\n",
        "      else:\n",
        "        self.weights = [\n",
        "              np.random.randn(input_size[0]*input_size[1], input_size[0]*input_size[1]),\n",
        "              np.random.randn(1, input_size[0]*input_size[1]),\n",
        "              np.random.randn(input_size[0]*input_size[1], layer_size),\n",
        "              np.random.randn(1, layer_size),\n",
        "#               np.random.randn(layer_size, layer_size),\n",
        "#               np.random.randn(1, layer_size),\n",
        "              np.random.randn(layer_size, output_size),\n",
        "          ]\n",
        "        \n",
        "    @njit()\n",
        "    def softmax(self, x):\n",
        "      e_x = np.exp(x - np.max(x, axis=-1, keepdims=True))\n",
        "      e_x = e_x / np.sum(e_x, axis=-1, keepdims=True)\n",
        "      return e_x\n",
        "    \n",
        "    @njit()\n",
        "    def predict(self, inputs):\n",
        "        inputs = inputs.flatten()\n",
        "        feed = np.dot(inputs, self.weights[0]) + self.weights[1]\n",
        "        softmax = self.softmax(feed)\n",
        "        mul = np.multiply(softmax,inputs)\n",
        "        feed = np.dot(mul, self.weights[2]) + self.weights[3]\n",
        "#         feed = np.dot(feed, self.weights[4]) + self.weights[5]\n",
        "        decision = np.dot(feed, self.weights[-1])\n",
        "        return decision\n",
        "\n",
        "    def get_weights(self):\n",
        "        return self.weights\n",
        "\n",
        "    def set_weights(self, weights):\n",
        "        self.weights = weights"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Q8BATs8Xyjwy",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class Agent:\n",
        "\n",
        "    POPULATION_SIZE = 100\n",
        "    SIGMA = 0.1\n",
        "    LEARNING_RATE = 0.01\n",
        "\n",
        "    def __init__(self, path, window_size, step_size ,spread=10, pip_cost=1000, los_cut=100, restore=False):\n",
        "        self.rand = np.random.RandomState()\n",
        "        self.path = path\n",
        "        self.window_size = window_size\n",
        "        self.step_size = step_size\n",
        "        self.spread = spread / pip_cost\n",
        "        self.pip_cost = pip_cost\n",
        "        self.los_cut = los_cut\n",
        "        self.preproc()\n",
        "        input_size = (window_size,self.df.shape[-1])\n",
        "        self.model = Model(input_size,128,3,restore)\n",
        "        self.rewards = reward3\n",
        "        self.es = Deep_Evolution_Strategy(\n",
        "            self.model.get_weights(),\n",
        "            self.get_reward,\n",
        "            self.test,\n",
        "            self.POPULATION_SIZE,\n",
        "            self.SIGMA,\n",
        "            self.LEARNING_RATE,\n",
        "        )\n",
        "\n",
        "    def act(self, sequence):\n",
        "        decision = self.model.predict(np.array(sequence))\n",
        "        return np.argmax(decision[0])\n",
        "      \n",
        "    def preproc(self):\n",
        "          self.dat = df = pd.read_csv(self.path)\n",
        "          s = np.asanyarray(ta.stoch(df[\"High\"],df[\"Low\"],df[\"Close\"],14)).reshape((-1, 1)) - np.asanyarray(ta.stoch_signal(df[\"High\"],df[\"Low\"],df[\"Close\"],14)).reshape((-1, 1))\n",
        "          m = np.asanyarray(ta.macd(df[\"Close\"])).reshape((-1, 1)) - np.asanyarray(ta.macd_signal(df[\"Close\"])).reshape((-1, 1))\n",
        "          trend3 = np.asanyarray(self.dat[[\"Close\"]]) - np.asanyarray(ta.ema(self.dat[\"Close\"],20)).reshape((-1, 1))\n",
        "          cross1 = np.asanyarray(ta.ema(self.dat[\"Close\"],20)).reshape((-1, 1)) - np.asanyarray(ta.ema(self.dat[\"Close\"],5)).reshape((-1, 1))\n",
        "          y = np.asanyarray(self.dat[[\"Open\"]])\n",
        "          x = s\n",
        "#           x = np.concatenate([s, m, trend3, cross1, np.asanyarray(df[[\"Close\"]])], 1)\n",
        "\n",
        "          gen = tf.keras.preprocessing.sequence.TimeseriesGenerator(x, y, self.window_size)\n",
        "          self.x = []\n",
        "          self.y = []\n",
        "          for i in gen:\n",
        "              self.x.extend(i[0].tolist())\n",
        "              self.y.extend(i[1].tolist())\n",
        "          self.x = np.asanyarray(self.x)\n",
        "          self.y = np.asanyarray(self.y)\n",
        "\n",
        "          self.df = self.x[-self.step_size::]\n",
        "          self.trend = self.y[-self.step_size::]\n",
        "\n",
        "    def get_reward(self, weights, change):\n",
        "        self.model.weights = weights\n",
        "        states = []\n",
        "        pip = []\n",
        "        provisional_pip = 0.0\n",
        "        total_pip = 0.0\n",
        "        position = 3\n",
        "        if change:\n",
        "          h = self.rand.randint(self.x.shape[0]-(self.step_size+1))\n",
        "          self.df = self.x[h:h+self.step_size]\n",
        "          self.trend = self.y[h:h+self.step_size]\n",
        "        for t in range(0, len(self.trend) - 1):\n",
        "            action = self.act(self.df[t])\n",
        "            states,provisional_pip,position,total_pip = self.rewards(self.trend[t],pip,provisional_pip,action,position,states,self.pip_cost,self.spread,total_pip)\n",
        "        return total_pip * self.pip_cost\n",
        "      \n",
        "    def test(self, weights,i):\n",
        "        self.model.weights = weights\n",
        "        states = []\n",
        "        pip = []\n",
        "        history = []\n",
        "        h_p = []\n",
        "        provisional_pip = 0.0\n",
        "        total_pip = 0.0\n",
        "        position = 3\n",
        "        for t in range(0, len(self.trend) - 1):\n",
        "            action = self.act(self.df[t])\n",
        "            history.append(action)\n",
        "            states,provisional_pip,position,total_pip = self.rewards(self.trend[t],pip,provisional_pip,action,position,states,self.pip_cost,self.spread,total_pip)\n",
        "            h_p.append(position)\n",
        "        self.pip = np.asanyarray(provisional_pip) * self.pip_cost\n",
        "        self.pip = [p if p >= -self.los_cut else -self.los_cut for p in self.pip]\n",
        "        self.total_pip = np.sum(self.pip)\n",
        "        mean_pip = self.total_pip / (t + 1)\n",
        "        trade_accuracy = np.mean(np.asanyarray(self.pip) > 0)\n",
        "        self.trade = trade_accuracy\n",
        "        mean_pip *= 24\n",
        "        prob = self.prob(history)\n",
        "        position_prob = self.prob(h_p)\n",
        "      \n",
        "        print(\"\")\n",
        "        print('action probability = ', prob)\n",
        "        print(\"buy = \", position_prob[1], \" sell = \", position_prob[-1])\n",
        "        print('trade accuracy = ', trade_accuracy)\n",
        "        print('epoch: %d, total rewards: %f, mean rewards: %f' % (i+1, float(self.total_pip), float(mean_pip)))\n",
        "        \n",
        "    def prob(self,history):\n",
        "        prob = np.asanyarray(history)\n",
        "        a = np.mean(prob == 0)\n",
        "        b = np.mean(prob == 1)\n",
        "        c = 1 - (a + b)\n",
        "        prob = [a,b,c]\n",
        "        return prob\n",
        "      \n",
        "    def fit(self, iterations, checkpoint):\n",
        "        self.es.train(iterations, print_every = checkpoint)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "b0NRq9_w8dtg",
        "colab_type": "code",
        "outputId": "d1c53570-b835-477d-8577-f168aa7ca327",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "window_size = 100\n",
        "step_size = 96\n",
        "path = \"audpred15.csv\"\n",
        "\n",
        "agent = Agent(path = path, \n",
        "              window_size = window_size,\n",
        "              step_size = step_size,\n",
        "              restore = False)\n",
        "agent.fit(iterations = 50000, checkpoint = 1)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\n",
            "action probability =  [0.3684210526315789, 0.24210526315789474, 0.3894736842105263]\n",
            "buy =  0.6421052631578947  sell =  0.35789473684210527\n",
            "trade accuracy =  0.3076923076923077\n",
            "epoch: 1, total rewards: -992.000000, mean rewards: -250.610526\n",
            "\n",
            "action probability =  [0.3473684210526316, 0.3894736842105263, 0.26315789473684204]\n",
            "buy =  0.45263157894736844  sell =  0.5473684210526315\n",
            "trade accuracy =  0.24\n",
            "epoch: 2, total rewards: -837.000000, mean rewards: -211.452632\n",
            "\n",
            "action probability =  [0.35789473684210527, 0.29473684210526313, 0.34736842105263155]\n",
            "buy =  0.5157894736842106  sell =  0.4842105263157894\n",
            "trade accuracy =  0.43103448275862066\n",
            "epoch: 3, total rewards: 2589.000000, mean rewards: 654.063158\n",
            "\n",
            "action probability =  [0.2631578947368421, 0.3263157894736842, 0.41052631578947363]\n",
            "buy =  0.4105263157894737  sell =  0.5894736842105264\n",
            "trade accuracy =  0.3225806451612903\n",
            "epoch: 4, total rewards: -530.000000, mean rewards: -133.894737\n",
            "\n",
            "action probability =  [0.42105263157894735, 0.24210526315789474, 0.33684210526315794]\n",
            "buy =  0.6421052631578947  sell =  0.35789473684210527\n",
            "trade accuracy =  0.32786885245901637\n",
            "epoch: 5, total rewards: -1072.000000, mean rewards: -270.821053\n",
            "\n",
            "action probability =  [0.3473684210526316, 0.35789473684210527, 0.2947368421052632]\n",
            "buy =  0.47368421052631576  sell =  0.5263157894736843\n",
            "trade accuracy =  0.4153846153846154\n",
            "epoch: 6, total rewards: 143.000000, mean rewards: 36.126316\n",
            "\n",
            "action probability =  [0.35789473684210527, 0.28421052631578947, 0.35789473684210527]\n",
            "buy =  0.5789473684210527  sell =  0.42105263157894735\n",
            "trade accuracy =  0.22033898305084745\n",
            "epoch: 7, total rewards: -1588.000000, mean rewards: -401.178947\n",
            "\n",
            "action probability =  [0.35789473684210527, 0.3368421052631579, 0.3052631578947369]\n",
            "buy =  0.5684210526315789  sell =  0.43157894736842106\n",
            "trade accuracy =  0.40298507462686567\n",
            "epoch: 8, total rewards: 1193.000000, mean rewards: 301.389474\n",
            "\n",
            "action probability =  [0.3368421052631579, 0.37894736842105264, 0.28421052631578947]\n",
            "buy =  0.4631578947368421  sell =  0.5368421052631579\n",
            "trade accuracy =  0.3728813559322034\n",
            "epoch: 9, total rewards: -627.000000, mean rewards: -158.400000\n",
            "\n",
            "action probability =  [0.3894736842105263, 0.2736842105263158, 0.33684210526315783]\n",
            "buy =  0.5473684210526316  sell =  0.4526315789473684\n",
            "trade accuracy =  0.29310344827586204\n",
            "epoch: 10, total rewards: -586.000000, mean rewards: -148.042105\n",
            "\n",
            "action probability =  [0.30526315789473685, 0.37894736842105264, 0.3157894736842105]\n",
            "buy =  0.43157894736842106  sell =  0.5684210526315789\n",
            "trade accuracy =  0.2833333333333333\n",
            "epoch: 11, total rewards: -561.000000, mean rewards: -141.726316\n",
            "\n",
            "action probability =  [0.3473684210526316, 0.25263157894736843, 0.3999999999999999]\n",
            "buy =  0.6  sell =  0.4\n",
            "trade accuracy =  0.532258064516129\n",
            "epoch: 12, total rewards: 3432.000000, mean rewards: 867.031579\n",
            "\n",
            "action probability =  [0.3684210526315789, 0.37894736842105264, 0.25263157894736843]\n",
            "buy =  0.49473684210526314  sell =  0.5052631578947369\n",
            "trade accuracy =  0.3968253968253968\n",
            "epoch: 13, total rewards: -187.000000, mean rewards: -47.242105\n",
            "\n",
            "action probability =  [0.3263157894736842, 0.3263157894736842, 0.34736842105263155]\n",
            "buy =  0.47368421052631576  sell =  0.5263157894736843\n",
            "trade accuracy =  0.4153846153846154\n",
            "epoch: 14, total rewards: 2816.000000, mean rewards: 711.410526\n",
            "\n",
            "action probability =  [0.42105263157894735, 0.3368421052631579, 0.24210526315789482]\n",
            "buy =  0.5684210526315789  sell =  0.43157894736842106\n",
            "trade accuracy =  0.4528301886792453\n",
            "epoch: 15, total rewards: 537.000000, mean rewards: 135.663158\n",
            "\n",
            "action probability =  [0.3157894736842105, 0.24210526315789474, 0.4421052631578948]\n",
            "buy =  0.5368421052631579  sell =  0.4631578947368421\n",
            "trade accuracy =  0.3188405797101449\n",
            "epoch: 16, total rewards: 128.000000, mean rewards: 32.336842\n",
            "\n",
            "action probability =  [0.3368421052631579, 0.37894736842105264, 0.28421052631578947]\n",
            "buy =  0.4842105263157895  sell =  0.5157894736842106\n",
            "trade accuracy =  0.3787878787878788\n",
            "epoch: 17, total rewards: -1064.000000, mean rewards: -268.800000\n",
            "\n",
            "action probability =  [0.4105263157894737, 0.28421052631578947, 0.3052631578947369]\n",
            "buy =  0.5789473684210527  sell =  0.42105263157894735\n",
            "trade accuracy =  0.3898305084745763\n",
            "epoch: 18, total rewards: -430.000000, mean rewards: -108.631579\n",
            "\n",
            "action probability =  [0.3263157894736842, 0.29473684210526313, 0.3789473684210527]\n",
            "buy =  0.5578947368421052  sell =  0.4421052631578948\n",
            "trade accuracy =  0.26229508196721313\n",
            "epoch: 19, total rewards: -471.000000, mean rewards: -118.989474\n",
            "\n",
            "action probability =  [0.3157894736842105, 0.3157894736842105, 0.368421052631579]\n",
            "buy =  0.5368421052631579  sell =  0.4631578947368421\n",
            "trade accuracy =  0.30303030303030304\n",
            "epoch: 20, total rewards: -558.000000, mean rewards: -140.968421\n",
            "\n",
            "action probability =  [0.3368421052631579, 0.30526315789473685, 0.35789473684210527]\n",
            "buy =  0.43157894736842106  sell =  0.5684210526315789\n",
            "trade accuracy =  0.3333333333333333\n",
            "epoch: 21, total rewards: -528.000000, mean rewards: -133.389474\n",
            "\n",
            "action probability =  [0.3263157894736842, 0.3157894736842105, 0.35789473684210527]\n",
            "buy =  0.5052631578947369  sell =  0.49473684210526314\n",
            "trade accuracy =  0.32857142857142857\n",
            "epoch: 22, total rewards: -1008.000000, mean rewards: -254.652632\n",
            "\n",
            "action probability =  [0.3157894736842105, 0.3368421052631579, 0.34736842105263155]\n",
            "buy =  0.5157894736842106  sell =  0.4842105263157894\n",
            "trade accuracy =  0.48484848484848486\n",
            "epoch: 23, total rewards: 519.000000, mean rewards: 131.115789\n",
            "\n",
            "action probability =  [0.35789473684210527, 0.28421052631578947, 0.35789473684210527]\n",
            "buy =  0.5578947368421052  sell =  0.4421052631578948\n",
            "trade accuracy =  0.30434782608695654\n",
            "epoch: 24, total rewards: -1196.000000, mean rewards: -302.147368\n",
            "\n",
            "action probability =  [0.3157894736842105, 0.3368421052631579, 0.34736842105263155]\n",
            "buy =  0.4842105263157895  sell =  0.5157894736842106\n",
            "trade accuracy =  0.4126984126984127\n",
            "epoch: 25, total rewards: 714.000000, mean rewards: 180.378947\n",
            "\n",
            "action probability =  [0.37894736842105264, 0.3157894736842105, 0.3052631578947369]\n",
            "buy =  0.5473684210526316  sell =  0.4526315789473684\n",
            "trade accuracy =  0.3181818181818182\n",
            "epoch: 26, total rewards: -592.000000, mean rewards: -149.557895\n",
            "\n",
            "action probability =  [0.4, 0.25263157894736843, 0.34736842105263155]\n",
            "buy =  0.6105263157894737  sell =  0.3894736842105263\n",
            "trade accuracy =  0.4067796610169492\n",
            "epoch: 27, total rewards: -808.000000, mean rewards: -204.126316\n",
            "\n",
            "action probability =  [0.3263157894736842, 0.3263157894736842, 0.34736842105263155]\n",
            "buy =  0.45263157894736844  sell =  0.5473684210526315\n",
            "trade accuracy =  0.3770491803278688\n",
            "epoch: 28, total rewards: 157.000000, mean rewards: 39.663158\n",
            "\n",
            "action probability =  [0.35789473684210527, 0.35789473684210527, 0.28421052631578947]\n",
            "buy =  0.5052631578947369  sell =  0.49473684210526314\n",
            "trade accuracy =  0.3770491803278688\n",
            "epoch: 29, total rewards: 25.000000, mean rewards: 6.315789\n",
            "\n",
            "action probability =  [0.3894736842105263, 0.28421052631578947, 0.3263157894736842]\n",
            "buy =  0.5684210526315789  sell =  0.43157894736842106\n",
            "trade accuracy =  0.5151515151515151\n",
            "epoch: 30, total rewards: 2534.000000, mean rewards: 640.168421\n",
            "\n",
            "action probability =  [0.3263157894736842, 0.37894736842105264, 0.2947368421052632]\n",
            "buy =  0.45263157894736844  sell =  0.5473684210526315\n",
            "trade accuracy =  0.4166666666666667\n",
            "epoch: 31, total rewards: 908.000000, mean rewards: 229.389474\n",
            "\n",
            "action probability =  [0.29473684210526313, 0.4105263157894737, 0.2947368421052632]\n",
            "buy =  0.37894736842105264  sell =  0.6210526315789473\n",
            "trade accuracy =  0.41379310344827586\n",
            "epoch: 32, total rewards: 1244.000000, mean rewards: 314.273684\n",
            "\n",
            "action probability =  [0.30526315789473685, 0.3263157894736842, 0.368421052631579]\n",
            "buy =  0.4421052631578947  sell =  0.5578947368421052\n",
            "trade accuracy =  0.38095238095238093\n",
            "epoch: 33, total rewards: 327.000000, mean rewards: 82.610526\n",
            "\n",
            "action probability =  [0.3263157894736842, 0.3263157894736842, 0.34736842105263155]\n",
            "buy =  0.47368421052631576  sell =  0.5263157894736843\n",
            "trade accuracy =  0.4827586206896552\n",
            "epoch: 34, total rewards: 2722.000000, mean rewards: 687.663158\n",
            "\n",
            "action probability =  [0.45263157894736844, 0.3473684210526316, 0.19999999999999996]\n",
            "buy =  0.5578947368421052  sell =  0.4421052631578948\n",
            "trade accuracy =  0.39215686274509803\n",
            "epoch: 35, total rewards: 130.000000, mean rewards: 32.842105\n",
            "\n",
            "action probability =  [0.30526315789473685, 0.3157894736842105, 0.3789473684210527]\n",
            "buy =  0.4421052631578947  sell =  0.5578947368421052\n",
            "trade accuracy =  0.43548387096774194\n",
            "epoch: 36, total rewards: 1545.000000, mean rewards: 390.315789\n",
            "\n",
            "action probability =  [0.3368421052631579, 0.2736842105263158, 0.3894736842105263]\n",
            "buy =  0.5368421052631579  sell =  0.4631578947368421\n",
            "trade accuracy =  0.3230769230769231\n",
            "epoch: 37, total rewards: 420.000000, mean rewards: 106.105263\n",
            "\n",
            "action probability =  [0.45263157894736844, 0.2736842105263158, 0.27368421052631575]\n",
            "buy =  0.5684210526315789  sell =  0.43157894736842106\n",
            "trade accuracy =  0.3548387096774194\n",
            "epoch: 38, total rewards: -266.000000, mean rewards: -67.200000\n",
            "\n",
            "action probability =  [0.4105263157894737, 0.29473684210526313, 0.2947368421052632]\n",
            "buy =  0.5473684210526316  sell =  0.4526315789473684\n",
            "trade accuracy =  0.23404255319148937\n",
            "epoch: 39, total rewards: -763.000000, mean rewards: -192.757895\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mXI1PwN_xpXT",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import pickle\n",
        "f = open('list.txt', 'wb')\n",
        "list_row = [(1,1,1,111),(1,1,1),(1,1,11)]\n",
        "pickle.dump(list_row, f)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WMUSUHl1yARm",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "f = open(\"./weights.txt\",\"rb\")\n",
        "list_row = pickle.load(f)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aBnhkjsoyBXt",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "np.array((list_row)).shape"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "O2nbHdKjza-g",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "weights = [\n",
        "  np.random.randn(30*2, 500),\n",
        "  np.random.randn(500, 3),\n",
        "  np.random.randn(1, 500),\n",
        "]\n",
        "\n",
        "def predict(inputs):\n",
        "  feed = np.dot(inputs, weights[0]) + weights[-1]\n",
        "  feed = feed.flatten().reshape((1,-1))\n",
        "  print(feed.shape)\n",
        "  decision = np.dot(feed, weights[1])\n",
        "  return decision"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0lIz3sfv-8_t",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# inputs = np.random.randn(30,2).flatten()\n",
        "predict(inputs)"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}