{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "dqac.ipynb のコピー",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/komo135/tradingrl/blob/master/dqac.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cBVV8hQizHTI",
        "colab_type": "code",
        "outputId": "45585a9b-0113-445b-8b9f-ff6675520147",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        }
      },
      "source": [
        "# Google ドライブをマウントするには、このセルを実行してください。\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "%cd drive/My Drive\n",
        "%load_ext Cython"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n",
            "/content/drive/My Drive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "prD6-LCFQYzW",
        "colab_type": "code",
        "outputId": "40d0cd81-4260-48d9-ba15-a5b8517eeacc",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 170
        }
      },
      "source": [
        "!pip install ta"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: ta in /usr/local/lib/python3.6/dist-packages (0.4.7)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.6/dist-packages (from ta) (0.25.2)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (from ta) (1.17.3)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.6/dist-packages (from ta) (0.21.3)\n",
            "Requirement already satisfied: pytz>=2017.2 in /usr/local/lib/python3.6/dist-packages (from pandas->ta) (2018.9)\n",
            "Requirement already satisfied: python-dateutil>=2.6.1 in /usr/local/lib/python3.6/dist-packages (from pandas->ta) (2.6.1)\n",
            "Requirement already satisfied: scipy>=0.17.0 in /usr/local/lib/python3.6/dist-packages (from scikit-learn->ta) (1.3.1)\n",
            "Requirement already satisfied: joblib>=0.11 in /usr/local/lib/python3.6/dist-packages (from scikit-learn->ta) (0.14.0)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.6/dist-packages (from python-dateutil>=2.6.1->pandas->ta) (1.12.0)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rfuucG8nABQX",
        "colab_type": "code",
        "outputId": "7d159432-f7d3-4166-b100-f93a974216b0",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 63
        }
      },
      "source": [
        "import numpy as np\n",
        "from scipy.special import expit\n",
        "import pandas as pd\n",
        "import pickle\n",
        "from numba import jit as njit\n",
        "from functools import lru_cache\n",
        "from multiprocessing import Pool\n",
        "import os\n",
        "import time\n",
        "import random\n",
        "import ta\n",
        "from net import *\n",
        "from memory import *\n",
        "from reward import *\n",
        "import traceback\n",
        "from IPython.display import clear_output"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "<p style=\"color: red;\">\n",
              "The default version of TensorFlow in Colab will soon switch to TensorFlow 2.x.<br>\n",
              "We recommend you <a href=\"https://www.tensorflow.org/guide/migrate\" target=\"_blank\">upgrade</a> now \n",
              "or ensure your notebook will continue to use TensorFlow 1.x via the <code>%tensorflow_version 1.x</code> magic:\n",
              "<a href=\"https://colab.research.google.com/notebooks/tensorflow_version.ipynb\" target=\"_blank\">more info</a>.</p>\n"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YkD_C2E8NaLX",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def swish(x):\n",
        "    x *= tf.nn.sigmoid(x)\n",
        "    return x\n",
        "\n",
        "def softmax(x):\n",
        "    \"\"\"Compute softmax values for each sets of scores in x.\"\"\"\n",
        "    e_x = np.exp(x - np.max(x))\n",
        "    return e_x / e_x.sum()\n",
        "\n",
        "def dqn(x):\n",
        "  with tf.variable_scope(\"dqn\", reuse=False):\n",
        "    x1 = tf.keras.layers.Conv1D(128,8,1,padding=\"causal\",activation=tf.nn.relu)(x)\n",
        "    x2 = tf.keras.layers.Conv1D(128,4,1,padding=\"causal\",activation=tf.nn.relu)(x)\n",
        "    x3 = tf.keras.layers.Conv1D(128,2,1,padding=\"causal\",activation=tf.nn.relu)(x)\n",
        "    x = tf.keras.layers.Concatenate()([x1,x2,x3])\n",
        "    x = tf.keras.layers.Conv1D(128,3,1,padding=\"valid\",activation=tf.nn.relu)(x)\n",
        "    f = tf.keras.layers.Flatten()(x)\n",
        "    out = tf.keras.layers.Dense(128,tf.nn.relu)(f)\n",
        "    out = tf.keras.layers.Dense(2)(out)\n",
        "    return out\n",
        "\n",
        "def actor(x,dqn):\n",
        "  with tf.variable_scope(\"actor\", reuse=False):\n",
        "    inputs = tf.keras.layers.Flatten()(x)\n",
        "    x1 = tf.keras.layers.Conv1D(128,8,1,padding=\"causal\",activation=tf.nn.relu)(x)\n",
        "    x2 = tf.keras.layers.Conv1D(128,4,1,padding=\"causal\",activation=tf.nn.relu)(x)\n",
        "    x3 = tf.keras.layers.Conv1D(128,2,1,padding=\"causal\",activation=tf.nn.relu)(x)\n",
        "    x = tf.keras.layers.Concatenate()([x1,x2,x3])\n",
        "    x = tf.keras.layers.Conv1D(128,3,1,padding=\"valid\",activation=tf.nn.relu)(x)\n",
        "    f = tf.keras.layers.Flatten()(x)\n",
        "    out = tf.keras.layers.Dense(128,tf.nn.relu)(f)\n",
        "    out = tf.keras.layers.Dense(2)(out)\n",
        "    out = out + dqn\n",
        "    out = tf.keras.layers.Dense(128,tf.nn.relu)(out)\n",
        "    shape = int(out.shape[-1])\n",
        "    a = tf.keras.layers.Dense(shape,tf.nn.softmax)(out)\n",
        "    mul = tf.keras.layers.Multiply()([out,a])\n",
        "    out = tf.keras.layers.Dense(128,tf.nn.relu)(mul)\n",
        "    out = tf.keras.layers.Dense(2,tf.tanh)(out)\n",
        "    return out\n",
        "\n",
        "def net(x,layer):\n",
        "  for i in layer:\n",
        "    x = i(x)\n",
        "  return x\n",
        "\n",
        "def critic(x,action,policy,q):\n",
        "  with tf.variable_scope(\"critic\", reuse=False):\n",
        "    x1 = tf.keras.layers.Conv1D(128,8,1,padding=\"causal\",activation=tf.nn.relu)(x)\n",
        "    x2 = tf.keras.layers.Conv1D(128,4,1,padding=\"causal\",activation=tf.nn.relu)(x)\n",
        "    x3 = tf.keras.layers.Conv1D(128,2,1,padding=\"causal\",activation=tf.nn.relu)(x)\n",
        "    x = tf.keras.layers.Concatenate()([x1,x2,x3])\n",
        "    x = tf.keras.layers.Conv1D(128,3,1,padding=\"valid\",activation=tf.nn.relu)(x)\n",
        "    f = tf.keras.layers.Flatten()(x)\n",
        "    # x = tf.keras.layers.Dense(128,tf.nn.relu)(f)\n",
        "    x1 = tf.keras.layers.Concatenate()([f,action,q])\n",
        "    x2 = tf.keras.layers.Concatenate()([f,policy,q])\n",
        "    layer = [tf.keras.layers.Dense(128,tf.nn.relu),tf.keras.layers.Dense(1)]\n",
        "    \n",
        "    x1 = net(x1,layer)\n",
        "    x2 = net(x2,layer)\n",
        "\n",
        "    return x1,x2"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rjO02597UgW9",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class Agent:\n",
        "    def __init__(self,path,window_siz):\n",
        "      self.path = path\n",
        "      self.window_size = window_size\n",
        "      self.STEP_SIZE = 20 * 12 * 2\n",
        "      self.preproc()\n",
        "      self.state_size = (None,self.window_size,self.df.shape[-1])\n",
        "      print(self.state_size)\n",
        "      self.memory = Memory(self.STEP_SIZE * 1000 * 5)\n",
        "      self.rewards = reward3\n",
        "      self.noise = 0.5\n",
        "      self.noise_min = 0.1\n",
        "      self.epsilon = 1.\n",
        "      self.DECAY_RATE = 0.005\n",
        "      self.min_epsilon = 0.1\n",
        "      self.sess = tf.Session()\n",
        "\n",
        "      with tf.variable_scope(\"input\"):\n",
        "        self.state = tf.placeholder(tf.float32, self.state_size)\n",
        "        self.new_state = tf.placeholder(tf.float32, self.state_size)\n",
        "        self.action = tf.placeholder(tf.float32,(None,2))\n",
        "        self.reward1 = tf.placeholder(tf.float32,(None,1))\n",
        "        self.reward2 = tf.placeholder(tf.float32,(None,2))\n",
        "\n",
        "      with tf.variable_scope(\"model\", reuse=False):\n",
        "        self.dqn = dqn(self.state)\n",
        "        self.policy = actor(self.state,self.dqn)\n",
        "        q,q_pi = critic(self.state,self.action,self.policy,self.dqn)\n",
        "      \n",
        "      with tf.variable_scope(\"target\", reuse=False):\n",
        "        self.target_dqn = dqn(self.new_state)\n",
        "\n",
        "      with tf.variable_scope(\"loss\"):\n",
        "        self.q_pi = q_pi\n",
        "        self.loss = loss = (0.5 * tf.reduce_mean((self.reward1 - q) ** 2))\n",
        "        dqn_loss = 0.5 * tf.reduce_mean((self.reward2 - self.dqn) ** 2)\n",
        "        self.ploss = policy_loss = -tf.reduce_mean(q_pi)\n",
        "        self.absolute_errors = tf.abs(self.reward1 - q)\n",
        "\n",
        "      self.actor_opt = tf.train.AdamOptimizer(1e-3).minimize(policy_loss, var_list=get_vars('model/actor'))\n",
        "      self.critic_opt = tf.train.AdamOptimizer(1e-3).minimize(loss, var_list=get_vars('model/critic'))\n",
        "      self.dqn_opt = tf.train.AdamOptimizer(1e-3).minimize(dqn_loss, var_list=get_vars('model/dqn'))\n",
        "\n",
        "      self.target_update = tf.group([tf.assign(v_targ, 0.001*v_targ + (1-0.001)*v_main)\n",
        "                                for v_main, v_targ in zip(get_vars('model/dqn'), get_vars('target/dqn'))])\n",
        "\n",
        "      target_init = tf.group([tf.assign(v_targ, v_main)\n",
        "                              for v_main, v_targ in zip(get_vars('model/dqn'), get_vars('target/dqn'))])\n",
        "\n",
        "      self.sess.run(tf.global_variables_initializer())\n",
        "      self.sess.run(target_init)\n",
        "\n",
        "    def preproc(self):\n",
        "        self.dat = df = pd.read_csv(self.path)\n",
        "        s = np.asanyarray(ta.stoch(df[\"High\"],df[\"Low\"],df[\"Close\"],14)).reshape((-1, 1)) - np.asanyarray(ta.stoch_signal(df[\"High\"],df[\"Low\"],df[\"Close\"],14)).reshape((-1, 1))\n",
        "        x = np.asanyarray(ta.daily_return(df[\"Close\"])).reshape((-1,1))\n",
        "        m = np.asanyarray(ta.macd_diff(df[\"Close\"])).reshape((-1,1))\n",
        "        cross1 = np.asanyarray(ta.ema(self.dat[\"Close\"],12)).reshape((-1, 1)) - np.asanyarray(ta.ema(self.dat[\"Close\"],5)).reshape((-1, 1))\n",
        "        trend = np.asanyarray(df[[\"Close\"]]) - np.asanyarray(ta.ema(self.dat[\"Close\"],50)).reshape((-1, 1))\n",
        "        # x = s\n",
        "        x = np.concatenate([s,x,cross1,trend,m], 1)\n",
        "        y = np.asanyarray(self.dat[[\"Open\"]])\n",
        "\n",
        "        gen = tf.keras.preprocessing.sequence.TimeseriesGenerator(x, y, self.window_size)\n",
        "        self.x = []\n",
        "        self.y = []\n",
        "        for i in gen:\n",
        "            self.x.extend(i[0].tolist())\n",
        "            self.y.extend(i[1].tolist())\n",
        "        self.x = np.asanyarray(self.x)[100:] #.reshape((-1, self.window_size, x.shape[-1]))\n",
        "        self.y = np.asanyarray(self.y)[100:]\n",
        "\n",
        "        self.df = self.x[-self.STEP_SIZE:]\n",
        "        self.trend = self.y[-self.STEP_SIZE:]\n",
        "\n",
        "    def _construct_memories_and_train(self,i, replay=None):\n",
        "      try:\n",
        "          tree_idx, replay = self.memory.sample(128)\n",
        "      except:\n",
        "          self.memory = Memory(5000)\n",
        "\n",
        "      states = np.array([a[0][0] for a in replay])\n",
        "      new_states = np.array([a[0][3] for a in replay])\n",
        "      actions = np.array([a[0][1] for a in replay]).reshape((-1, 2))\n",
        "      act = np.array([a[0][-2] for a in replay]).reshape((-1, 1))\n",
        "      rewards = np.array([a[0][2] for a in replay]).reshape((-1, 1))\n",
        "      r = rewards.copy()\n",
        "      done = np.array([a[0][-1] for a in replay]).reshape((-1, 1))\n",
        "\n",
        "      q = self.sess.run(self.dqn,feed_dict={self.state:states,self.action:actions})\n",
        "      new_q = self.sess.run(self.dqn,feed_dict={self.state:new_states})\n",
        "      new_target_q = self.sess.run(self.target_dqn, feed_dict={self.new_state:new_states})\n",
        "\n",
        "      for I in range(128):\n",
        "        q[I,act[I]] = r[I] + done[I]# * 0.99 * new_target_q[I,np.argmax(new_q[I])]\n",
        "        rewards[I] = rewards[I] + done[I] * 0.99 * new_target_q[I,np.argmax(new_q[I])]\n",
        "\n",
        "      step_ops = [self.absolute_errors,self.critic_opt,self.dqn_opt]\n",
        "      for _ in range(1):\n",
        "        absolute_errors,_,_ = self.sess.run(step_ops, feed_dict={self.state: states, self.reward1:rewards ,self.reward2:q ,self.action:actions})\n",
        "      if i > 20:\n",
        "        if (i+1) % 2 == 0:\n",
        "              loss,_ = self.sess.run([self.loss,self.actor_opt], feed_dict={self.state: states,self.reward1:rewards, self.action:actions})\n",
        "              # print(loss)\n",
        "          # print([rewards,loss])\n",
        "      self.sess.run(self.target_update)\n",
        "      self.memory.batch_update(tree_idx, absolute_errors)\n",
        "\n",
        "    def prob(self,history):\n",
        "        prob = np.asanyarray(history)\n",
        "        a = np.mean(prob == 0)\n",
        "        b = np.mean(prob == 1)\n",
        "        c = 1 - (a + b)\n",
        "        prob = [a,b,c]\n",
        "        return prob\n",
        "\n",
        "    def discount_rewards(self, r, running_add):\n",
        "        running_add = running_add * 0.99 + r\n",
        "        return running_add\n",
        "        \n",
        "    def nstep(self,r):\n",
        "        running_add = 0.0\n",
        "        for t in range(len(r)):\n",
        "            running_add += 0.99 * r[t]\n",
        "\n",
        "        return running_add\n",
        "\n",
        "    def _select_action(self, state, i):\n",
        "      prediction,q = self.sess.run([self.policy,self.dqn], feed_dict={self.state: [state]})\n",
        "      prediction,q = prediction[0],q[0]\n",
        "      q = np.abs(q) / np.sum(q)\n",
        "      if i < 20:\n",
        "        self.noise = 1.\n",
        "      elif i == 20:\n",
        "        self.noise = 0.5\n",
        "\n",
        "      if self.noise > self.noise_min:\n",
        "        self.noise *= 0.9999\n",
        "      noise = 0. if (i + 1) % 5 == 0 else self.noise\n",
        "      # print(prediction)\n",
        "      prediction += noise * np.random.randn(2)\n",
        "      q += noise * np.random.randn(2)\n",
        "      q += prediction\n",
        "      prediction = np.clip(prediction, -1, 1)\n",
        "      noise = 0. if (i + 1) % 5 == 0 else 0.1\n",
        "      q = (np.abs(q) / np.sum(q)) + noise * np.random.randn(2)\n",
        "      # prediction = q\n",
        "      action = np.argmax(q)\n",
        "      self.pred = prediction\n",
        "\n",
        "      return action\n",
        "\n",
        "    def run(self, spread=10, pip_cost=1000, los_cut=600, day_pip=20, n=10,step=100):\n",
        "        spread = spread / pip_cost\n",
        "        self.rand = np.random.RandomState()\n",
        "        lc = los_cut / pip_cost\n",
        "        # h = self.rand.randint(self.x.shape[0]-(self.STEP_SIZE+1))\n",
        "        # self.df = self.x[h:h+self.STEP_SIZE]\n",
        "        # self.trend = self.y[h:h+self.STEP_SIZE]\n",
        "        for i in range(100000):\n",
        "            # if i % 1000 == 0:\n",
        "            #   h = self.rand.randint(self.x.shape[0]-(self.STEP_SIZE+1))\n",
        "            #   self.df = self.x[h:h+self.STEP_SIZE]\n",
        "            #   self.trend = self.y[h:h+self.STEP_SIZE]\n",
        "            done = 1.0\n",
        "            position = 3\n",
        "            pip = []\n",
        "            provisional_pip = []\n",
        "            running_add = 0.0\n",
        "            total_pip = 0.0\n",
        "            old_reword = 0.0\n",
        "            states = []\n",
        "            h_a = []\n",
        "            h_a1 = []\n",
        "            h_r = []\n",
        "            h_p = []\n",
        "            old = np.asanyarray(0)\n",
        "            self.history = []\n",
        "            for t in  range(0, len(self.trend)-1):\n",
        "                action = self._select_action(self.df[t],i)\n",
        "                h_a.append(self.pred)\n",
        "                h_a1.append(action)\n",
        "                self.history.append(action)\n",
        "                \n",
        "                states,provisional_pip,position,total_pip = self.rewards(self.trend[t],pip,provisional_pip,\n",
        "                                    action,position,states,pip_cost,spread,total_pip,lc=min([lc/2,100/pip_cost]))\n",
        "                h_p.append(position)\n",
        "                reward =  (total_pip - old_reword) * 100\n",
        "                old_reword = total_pip\n",
        "                h_r.append(reward)\n",
        "\n",
        "                # running_add = self.discount_rewards(reward,running_add)\n",
        "                # r_d = int(running_add * (pip_cost / 100)) * 100\n",
        "                # # r_d = int(np.mean(np.array(provisional_pip) > 0)*100) if len(provisional_pip) != 0 else 0.\n",
        "                # if t == len(self.trend)-5:\n",
        "                #     done = 0.\n",
        "                # exp = self.df[t], h_a[t], r_d, self.df[t+1], done\n",
        "                # self.memory.store(exp)\n",
        "            self.epsilon = self.min_epsilon + (1.0 - self.min_epsilon) * np.exp(-self.DECAY_RATE * i)\n",
        "            for t in range(0, len(self.trend)-1):\n",
        "                tau = t - n + 1\n",
        "                if tau >= 0:\n",
        "                    rewards = self.nstep(h_r[tau+1:tau+n])\n",
        "                    exp = self.df[tau], h_a[tau], int(rewards), self.df[tau+n], h_a1[tau], done\n",
        "                    self.memory.store(exp)\n",
        "\n",
        "            try:\n",
        "              self._construct_memories_and_train(i)\n",
        "            except:\n",
        "              traceback.print_exc()\n",
        "\n",
        "            if i % 1000 == 0:\n",
        "              clear_output()\n",
        "\n",
        "            if (i + 1) % 5 == 0:\n",
        "                # loss = np.mean(ae)\n",
        "                self.pip = np.asanyarray(provisional_pip) * pip_cost\n",
        "                self.pip = [p if p >= -los_cut else -los_cut for p in self.pip]\n",
        "                self.total_pip = np.sum(self.pip)\n",
        "                mean_pip = self.total_pip / (t + 1)\n",
        "                trade_accuracy = np.mean(np.asanyarray(self.pip) > 0)\n",
        "                self.trade = trade_accuracy\n",
        "                mean_pip *= day_pip\n",
        "                prob = self.prob(self.history)\n",
        "                position_prob = self.prob(h_p)\n",
        "\n",
        "                # print(\"loss =\", loss)\n",
        "                # print(\"\")\n",
        "                print('action probability = ', prob)\n",
        "                print(\"buy = \", position_prob[1], \" sell = \", position_prob[-1])\n",
        "                print('trade accuracy = ', trade_accuracy)\n",
        "                print('epoch: %d, total rewards: %f, mean rewards: %f' % (i + 1, float(self.total_pip), float(mean_pip)))\n",
        "                print(\"\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sxh1Lhc_MIpT",
        "colab_type": "code",
        "outputId": "77043f1f-ce37-4a01-f683-93a5305d0d28",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "window_size = 30\n",
        "path = \"audpred1440.csv\"\n",
        "agent = Agent(path,window_size)\n",
        "\n",
        "agent.run()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "action probability =  [0.42379958246346555, 0.5762004175365344, 0.0]\n",
            "buy =  0.42379958246346555  sell =  0.5762004175365345\n",
            "trade accuracy =  0.9300847457627118\n",
            "epoch: 8005, total rewards: 1213119.000000, mean rewards: 50652.150313\n",
            "\n",
            "action probability =  [0.42171189979123175, 0.5782881002087683, 0.0]\n",
            "buy =  0.42171189979123175  sell =  0.5782881002087683\n",
            "trade accuracy =  0.9426751592356688\n",
            "epoch: 8010, total rewards: 1214415.000000, mean rewards: 50706.263048\n",
            "\n",
            "action probability =  [0.4363256784968685, 0.5636743215031316, 0.0]\n",
            "buy =  0.4363256784968685  sell =  0.5636743215031315\n",
            "trade accuracy =  0.9531914893617022\n",
            "epoch: 8015, total rewards: 1226078.000000, mean rewards: 51193.235908\n",
            "\n",
            "action probability =  [0.4551148225469729, 0.5448851774530271, 0.0]\n",
            "buy =  0.4551148225469729  sell =  0.5448851774530271\n",
            "trade accuracy =  0.9553191489361702\n",
            "epoch: 8020, total rewards: 1226709.000000, mean rewards: 51219.582463\n",
            "\n",
            "action probability =  [0.4592901878914405, 0.5407098121085595, 0.0]\n",
            "buy =  0.4592901878914405  sell =  0.5407098121085595\n",
            "trade accuracy =  0.9530916844349681\n",
            "epoch: 8025, total rewards: 1225526.000000, mean rewards: 51170.187891\n",
            "\n",
            "action probability =  [0.4509394572025052, 0.5490605427974948, 0.0]\n",
            "buy =  0.4509394572025052  sell =  0.5490605427974948\n",
            "trade accuracy =  0.9508547008547008\n",
            "epoch: 8030, total rewards: 1224911.000000, mean rewards: 51144.509395\n",
            "\n",
            "action probability =  [0.4592901878914405, 0.5407098121085595, 0.0]\n",
            "buy =  0.4592901878914405  sell =  0.5407098121085595\n",
            "trade accuracy =  0.9313304721030042\n",
            "epoch: 8035, total rewards: 1068755.000000, mean rewards: 44624.425887\n",
            "\n",
            "action probability =  [0.49478079331941544, 0.5052192066805845, 0.0]\n",
            "buy =  0.49478079331941544  sell =  0.5052192066805845\n",
            "trade accuracy =  0.9143468950749465\n",
            "epoch: 8040, total rewards: 1053493.000000, mean rewards: 43987.181628\n",
            "\n",
            "action probability =  [0.5073068893528184, 0.49269311064718163, 0.0]\n",
            "buy =  0.5073068893528184  sell =  0.4926931106471816\n",
            "trade accuracy =  0.8843683083511777\n",
            "epoch: 8045, total rewards: 1039582.000000, mean rewards: 43406.346555\n",
            "\n",
            "action probability =  [0.5031315240083507, 0.4968684759916493, 0.0]\n",
            "buy =  0.5031315240083507  sell =  0.4968684759916493\n",
            "trade accuracy =  0.8907922912205567\n",
            "epoch: 8050, total rewards: 1037237.000000, mean rewards: 43308.434238\n",
            "\n",
            "action probability =  [0.4968684759916493, 0.5031315240083507, 0.0]\n",
            "buy =  0.4968684759916493  sell =  0.5031315240083507\n",
            "trade accuracy =  0.8993576017130621\n",
            "epoch: 8055, total rewards: 1047191.000000, mean rewards: 43724.050104\n",
            "\n",
            "action probability =  [0.4592901878914405, 0.5407098121085595, 0.0]\n",
            "buy =  0.4592901878914405  sell =  0.5407098121085595\n",
            "trade accuracy =  0.9444444444444444\n",
            "epoch: 8060, total rewards: 1076069.000000, mean rewards: 44929.812109\n",
            "\n",
            "action probability =  [0.4342379958246347, 0.5657620041753654, 0.0]\n",
            "buy =  0.4342379958246347  sell =  0.5657620041753653\n",
            "trade accuracy =  0.9637526652452025\n",
            "epoch: 8065, total rewards: 1229817.000000, mean rewards: 51349.352818\n",
            "\n",
            "action probability =  [0.4279749478079332, 0.5720250521920668, 0.0]\n",
            "buy =  0.4279749478079332  sell =  0.5720250521920668\n",
            "trade accuracy =  0.9530916844349681\n",
            "epoch: 8070, total rewards: 1225530.000000, mean rewards: 51170.354906\n",
            "\n",
            "action probability =  [0.4321503131524008, 0.5678496868475992, 0.0]\n",
            "buy =  0.4321503131524008  sell =  0.5678496868475992\n",
            "trade accuracy =  0.9594882729211087\n",
            "epoch: 8075, total rewards: 1228871.000000, mean rewards: 51309.853862\n",
            "\n",
            "action probability =  [0.4342379958246347, 0.5657620041753654, 0.0]\n",
            "buy =  0.4342379958246347  sell =  0.5657620041753653\n",
            "trade accuracy =  0.9572649572649573\n",
            "epoch: 8080, total rewards: 1225940.000000, mean rewards: 51187.473904\n",
            "\n",
            "action probability =  [0.44258872651356995, 0.55741127348643, 0.0]\n",
            "buy =  0.44258872651356995  sell =  0.55741127348643\n",
            "trade accuracy =  0.9616204690831557\n",
            "epoch: 8085, total rewards: 1224725.000000, mean rewards: 51136.743215\n",
            "\n",
            "action probability =  [0.4551148225469729, 0.5448851774530271, 0.0]\n",
            "buy =  0.4551148225469729  sell =  0.5448851774530271\n",
            "trade accuracy =  0.9680851063829787\n",
            "epoch: 8090, total rewards: 1226718.000000, mean rewards: 51219.958246\n",
            "\n",
            "action probability =  [0.4551148225469729, 0.5448851774530271, 0.0]\n",
            "buy =  0.4551148225469729  sell =  0.5448851774530271\n",
            "trade accuracy =  0.9639065817409767\n",
            "epoch: 8095, total rewards: 1225359.000000, mean rewards: 51163.215031\n",
            "\n",
            "action probability =  [0.4613778705636743, 0.5386221294363257, 0.0]\n",
            "buy =  0.4613778705636743  sell =  0.5386221294363257\n",
            "trade accuracy =  0.9596602972399151\n",
            "epoch: 8100, total rewards: 1224801.000000, mean rewards: 51139.916493\n",
            "\n",
            "action probability =  [0.47390396659707723, 0.5260960334029228, 0.0]\n",
            "buy =  0.47390396659707723  sell =  0.5260960334029228\n",
            "trade accuracy =  0.9531914893617022\n",
            "epoch: 8105, total rewards: 1220005.000000, mean rewards: 50939.665971\n",
            "\n",
            "action probability =  [0.46555323590814196, 0.534446764091858, 0.0]\n",
            "buy =  0.46555323590814196  sell =  0.534446764091858\n",
            "trade accuracy =  0.9596602972399151\n",
            "epoch: 8110, total rewards: 1225369.000000, mean rewards: 51163.632568\n",
            "\n",
            "action probability =  [0.44467640918580376, 0.5553235908141962, 0.0]\n",
            "buy =  0.44467640918580376  sell =  0.5553235908141962\n",
            "trade accuracy =  0.9553191489361702\n",
            "epoch: 8115, total rewards: 1224685.000000, mean rewards: 51135.073069\n",
            "\n",
            "action probability =  [0.4196242171189979, 0.5803757828810021, 0.0]\n",
            "buy =  0.4196242171189979  sell =  0.5803757828810021\n",
            "trade accuracy =  0.9468085106382979\n",
            "epoch: 8120, total rewards: 1224481.000000, mean rewards: 51126.555324\n",
            "\n",
            "action probability =  [0.4175365344467641, 0.5824634655532359, 0.0]\n",
            "buy =  0.4175365344467641  sell =  0.5824634655532359\n",
            "trade accuracy =  0.929637526652452\n",
            "epoch: 8125, total rewards: 1213575.000000, mean rewards: 50671.189979\n",
            "\n",
            "action probability =  [0.44467640918580376, 0.5553235908141962, 0.0]\n",
            "buy =  0.44467640918580376  sell =  0.5553235908141962\n",
            "trade accuracy =  0.9616204690831557\n",
            "epoch: 8130, total rewards: 1214159.000000, mean rewards: 50695.574113\n",
            "\n",
            "action probability =  [0.46764091858037576, 0.5323590814196242, 0.0]\n",
            "buy =  0.46764091858037576  sell =  0.5323590814196242\n",
            "trade accuracy =  0.9509594882729211\n",
            "epoch: 8135, total rewards: 1143651.000000, mean rewards: 47751.607516\n",
            "\n",
            "action probability =  [0.4759916492693111, 0.524008350730689, 0.0]\n",
            "buy =  0.4759916492693111  sell =  0.524008350730689\n",
            "trade accuracy =  0.948936170212766\n",
            "epoch: 8140, total rewards: 1142376.000000, mean rewards: 47698.371608\n",
            "\n",
            "action probability =  [0.4822546972860125, 0.5177453027139874, 0.0]\n",
            "buy =  0.4822546972860125  sell =  0.5177453027139876\n",
            "trade accuracy =  0.9446808510638298\n",
            "epoch: 8145, total rewards: 1198833.000000, mean rewards: 50055.657620\n",
            "\n",
            "action probability =  [0.4822546972860125, 0.5177453027139874, 0.0]\n",
            "buy =  0.4822546972860125  sell =  0.5177453027139876\n",
            "trade accuracy =  0.9404255319148936\n",
            "epoch: 8150, total rewards: 1195706.000000, mean rewards: 49925.093946\n",
            "\n",
            "action probability =  [0.46555323590814196, 0.534446764091858, 0.0]\n",
            "buy =  0.46555323590814196  sell =  0.534446764091858\n",
            "trade accuracy =  0.9468085106382979\n",
            "epoch: 8155, total rewards: 1202362.000000, mean rewards: 50203.006263\n",
            "\n",
            "action probability =  [0.4363256784968685, 0.5636743215031316, 0.0]\n",
            "buy =  0.4363256784968685  sell =  0.5636743215031315\n",
            "trade accuracy =  0.9466950959488273\n",
            "epoch: 8160, total rewards: 1225225.000000, mean rewards: 51157.620042\n",
            "\n",
            "action probability =  [0.4112734864300626, 0.5887265135699373, 0.0]\n",
            "buy =  0.4112734864300626  sell =  0.5887265135699373\n",
            "trade accuracy =  0.9381663113006397\n",
            "epoch: 8165, total rewards: 1214349.000000, mean rewards: 50703.507307\n",
            "\n",
            "action probability =  [0.3945720250521921, 0.605427974947808, 0.0]\n",
            "buy =  0.3945720250521921  sell =  0.605427974947808\n",
            "trade accuracy =  0.9317697228144989\n",
            "epoch: 8170, total rewards: 1213019.000000, mean rewards: 50647.974948\n",
            "\n",
            "action probability =  [0.40083507306889354, 0.5991649269311065, 0.0]\n",
            "buy =  0.40083507306889354  sell =  0.5991649269311065\n",
            "trade accuracy =  0.9401709401709402\n",
            "epoch: 8175, total rewards: 1214029.000000, mean rewards: 50690.146138\n",
            "\n",
            "action probability =  [0.40501043841336115, 0.5949895615866388, 0.0]\n",
            "buy =  0.40501043841336115  sell =  0.5949895615866388\n",
            "trade accuracy =  0.9317697228144989\n",
            "epoch: 8180, total rewards: 1208316.000000, mean rewards: 50451.607516\n",
            "\n",
            "action probability =  [0.407098121085595, 0.592901878914405, 0.0]\n",
            "buy =  0.407098121085595  sell =  0.5929018789144049\n",
            "trade accuracy =  0.9339019189765458\n",
            "epoch: 8185, total rewards: 1209513.000000, mean rewards: 50501.586639\n",
            "\n",
            "action probability =  [0.4133611691022965, 0.5866388308977035, 0.0]\n",
            "buy =  0.4133611691022965  sell =  0.5866388308977035\n",
            "trade accuracy =  0.9423076923076923\n",
            "epoch: 8190, total rewards: 1214808.000000, mean rewards: 50722.672234\n",
            "\n",
            "action probability =  [0.42379958246346555, 0.5762004175365344, 0.0]\n",
            "buy =  0.42379958246346555  sell =  0.5762004175365345\n",
            "trade accuracy =  0.9529914529914529\n",
            "epoch: 8195, total rewards: 1213720.000000, mean rewards: 50677.244259\n",
            "\n",
            "action probability =  [0.42171189979123175, 0.5782881002087683, 0.0]\n",
            "buy =  0.42171189979123175  sell =  0.5782881002087683\n",
            "trade accuracy =  0.9528907922912205\n",
            "epoch: 8200, total rewards: 1215474.000000, mean rewards: 50750.480167\n",
            "\n",
            "action probability =  [0.44258872651356995, 0.55741127348643, 0.0]\n",
            "buy =  0.44258872651356995  sell =  0.55741127348643\n",
            "trade accuracy =  0.9488272921108742\n",
            "epoch: 8205, total rewards: 1219447.000000, mean rewards: 50916.367432\n",
            "\n",
            "action probability =  [0.46764091858037576, 0.5323590814196242, 0.0]\n",
            "buy =  0.46764091858037576  sell =  0.5323590814196242\n",
            "trade accuracy =  0.948936170212766\n",
            "epoch: 8210, total rewards: 1215487.000000, mean rewards: 50751.022965\n",
            "\n",
            "action probability =  [0.48434237995824636, 0.5156576200417536, 0.0]\n",
            "buy =  0.48434237995824636  sell =  0.5156576200417536\n",
            "trade accuracy =  0.9253731343283582\n",
            "epoch: 8215, total rewards: 1201048.000000, mean rewards: 50148.141962\n",
            "\n",
            "action probability =  [0.4822546972860125, 0.5177453027139874, 0.0]\n",
            "buy =  0.4822546972860125  sell =  0.5177453027139876\n",
            "trade accuracy =  0.9360341151385928\n",
            "epoch: 8220, total rewards: 1204931.000000, mean rewards: 50310.271399\n",
            "\n",
            "action probability =  [0.46555323590814196, 0.534446764091858, 0.0]\n",
            "buy =  0.46555323590814196  sell =  0.534446764091858\n",
            "trade accuracy =  0.9531914893617022\n",
            "epoch: 8225, total rewards: 1213767.000000, mean rewards: 50679.206681\n",
            "\n",
            "action probability =  [0.4405010438413361, 0.5594989561586639, 0.0]\n",
            "buy =  0.4405010438413361  sell =  0.5594989561586639\n",
            "trade accuracy =  0.9552238805970149\n",
            "epoch: 8230, total rewards: 1215821.000000, mean rewards: 50764.968685\n",
            "\n",
            "action probability =  [0.4196242171189979, 0.5803757828810021, 0.0]\n",
            "buy =  0.4196242171189979  sell =  0.5803757828810021\n",
            "trade accuracy =  0.9488272921108742\n",
            "epoch: 8235, total rewards: 1213535.000000, mean rewards: 50669.519833\n",
            "\n",
            "action probability =  [0.4321503131524008, 0.5678496868475992, 0.0]\n",
            "buy =  0.4321503131524008  sell =  0.5678496868475992\n",
            "trade accuracy =  0.9488272921108742\n",
            "epoch: 8240, total rewards: 1212606.000000, mean rewards: 50630.730689\n",
            "\n",
            "action probability =  [0.46346555323590816, 0.5365344467640919, 0.0]\n",
            "buy =  0.46346555323590816  sell =  0.5365344467640918\n",
            "trade accuracy =  0.948936170212766\n",
            "epoch: 8245, total rewards: 1206381.000000, mean rewards: 50370.814196\n",
            "\n",
            "action probability =  [0.4718162839248434, 0.5281837160751566, 0.0]\n",
            "buy =  0.4718162839248434  sell =  0.5281837160751566\n",
            "trade accuracy =  0.9382978723404255\n",
            "epoch: 8250, total rewards: 1201308.000000, mean rewards: 50158.997912\n",
            "\n",
            "action probability =  [0.4613778705636743, 0.5386221294363257, 0.0]\n",
            "buy =  0.4613778705636743  sell =  0.5386221294363257\n",
            "trade accuracy =  0.9490445859872612\n",
            "epoch: 8255, total rewards: 1210088.000000, mean rewards: 50525.594990\n",
            "\n",
            "action probability =  [0.4363256784968685, 0.5636743215031316, 0.0]\n",
            "buy =  0.4363256784968685  sell =  0.5636743215031315\n",
            "trade accuracy =  0.9574468085106383\n",
            "epoch: 8260, total rewards: 1229026.000000, mean rewards: 51316.325678\n",
            "\n",
            "action probability =  [0.4279749478079332, 0.5720250521920668, 0.0]\n",
            "buy =  0.4279749478079332  sell =  0.5720250521920668\n",
            "trade accuracy =  0.9552238805970149\n",
            "epoch: 8265, total rewards: 1226153.000000, mean rewards: 51196.367432\n",
            "\n",
            "action probability =  [0.42588726513569936, 0.5741127348643006, 0.0]\n",
            "buy =  0.42588726513569936  sell =  0.5741127348643007\n",
            "trade accuracy =  0.9445628997867804\n",
            "epoch: 8270, total rewards: 1219361.000000, mean rewards: 50912.776618\n",
            "\n",
            "action probability =  [0.40501043841336115, 0.5949895615866388, 0.0]\n",
            "buy =  0.40501043841336115  sell =  0.5949895615866388\n",
            "trade accuracy =  0.9275053304904051\n",
            "epoch: 8275, total rewards: 1215888.000000, mean rewards: 50767.766180\n",
            "\n",
            "action probability =  [0.3945720250521921, 0.605427974947808, 0.0]\n",
            "buy =  0.3945720250521921  sell =  0.605427974947808\n",
            "trade accuracy =  0.9212765957446809\n",
            "epoch: 8280, total rewards: 1211872.000000, mean rewards: 50600.083507\n",
            "\n",
            "action probability =  [0.4133611691022965, 0.5866388308977035, 0.0]\n",
            "buy =  0.4133611691022965  sell =  0.5866388308977035\n",
            "trade accuracy =  0.938034188034188\n",
            "epoch: 8285, total rewards: 1217405.000000, mean rewards: 50831.106472\n",
            "\n",
            "action probability =  [0.4551148225469729, 0.5448851774530271, 0.0]\n",
            "buy =  0.4551148225469729  sell =  0.5448851774530271\n",
            "trade accuracy =  0.9552238805970149\n",
            "epoch: 8290, total rewards: 1220352.000000, mean rewards: 50954.154489\n",
            "\n",
            "action probability =  [0.48643006263048016, 0.5135699373695198, 0.0]\n",
            "buy =  0.48643006263048016  sell =  0.5135699373695198\n",
            "trade accuracy =  0.9363057324840764\n",
            "epoch: 8295, total rewards: 1188291.000000, mean rewards: 49615.490605\n",
            "\n",
            "action probability =  [0.47390396659707723, 0.5260960334029228, 0.0]\n",
            "buy =  0.47390396659707723  sell =  0.5260960334029228\n",
            "trade accuracy =  0.940552016985138\n",
            "epoch: 8300, total rewards: 1195752.000000, mean rewards: 49927.014614\n",
            "\n",
            "action probability =  [0.42588726513569936, 0.5741127348643006, 0.0]\n",
            "buy =  0.42588726513569936  sell =  0.5741127348643007\n",
            "trade accuracy =  0.948936170212766\n",
            "epoch: 8305, total rewards: 1219852.000000, mean rewards: 50933.277662\n",
            "\n",
            "action probability =  [0.38413361169102295, 0.615866388308977, 0.0]\n",
            "buy =  0.38413361169102295  sell =  0.615866388308977\n",
            "trade accuracy =  0.9278131634819533\n",
            "epoch: 8310, total rewards: 1208349.000000, mean rewards: 50452.985386\n",
            "\n",
            "action probability =  [0.3465553235908142, 0.6534446764091858, 0.0]\n",
            "buy =  0.3465553235908142  sell =  0.6534446764091858\n",
            "trade accuracy =  0.9\n",
            "epoch: 8315, total rewards: 1185814.000000, mean rewards: 49512.066806\n",
            "\n",
            "action probability =  [0.38204592901878914, 0.6179540709812108, 0.0]\n",
            "buy =  0.38204592901878914  sell =  0.6179540709812108\n",
            "trade accuracy =  0.9360341151385928\n",
            "epoch: 8320, total rewards: 1211673.000000, mean rewards: 50591.774530\n",
            "\n",
            "action probability =  [0.42379958246346555, 0.5762004175365344, 0.0]\n",
            "buy =  0.42379958246346555  sell =  0.5762004175365345\n",
            "trade accuracy =  0.951063829787234\n",
            "epoch: 8325, total rewards: 1223349.000000, mean rewards: 51079.290188\n",
            "\n",
            "action probability =  [0.46346555323590816, 0.5365344467640919, 0.0]\n",
            "buy =  0.46346555323590816  sell =  0.5365344467640918\n",
            "trade accuracy =  0.9424307036247335\n",
            "epoch: 8330, total rewards: 1218402.000000, mean rewards: 50872.734864\n",
            "\n",
            "action probability =  [0.5135699373695198, 0.48643006263048016, 0.0]\n",
            "buy =  0.5135699373695198  sell =  0.48643006263048016\n",
            "trade accuracy =  0.8931623931623932\n",
            "epoch: 8335, total rewards: 1030981.000000, mean rewards: 43047.223382\n",
            "\n",
            "action probability =  [0.4968684759916493, 0.5031315240083507, 0.0]\n",
            "buy =  0.4968684759916493  sell =  0.5031315240083507\n",
            "trade accuracy =  0.9102564102564102\n",
            "epoch: 8340, total rewards: 1045460.000000, mean rewards: 43651.774530\n",
            "\n",
            "action probability =  [0.4759916492693111, 0.524008350730689, 0.0]\n",
            "buy =  0.4759916492693111  sell =  0.524008350730689\n",
            "trade accuracy =  0.9252136752136753\n",
            "epoch: 8345, total rewards: 1058538.000000, mean rewards: 44197.828810\n",
            "\n",
            "action probability =  [0.4592901878914405, 0.5407098121085595, 0.0]\n",
            "buy =  0.4592901878914405  sell =  0.5407098121085595\n",
            "trade accuracy =  0.9337606837606838\n",
            "epoch: 8350, total rewards: 1065520.000000, mean rewards: 44489.352818\n",
            "\n",
            "action probability =  [0.4509394572025052, 0.5490605427974948, 0.0]\n",
            "buy =  0.4509394572025052  sell =  0.5490605427974948\n",
            "trade accuracy =  0.9189765458422174\n",
            "epoch: 8355, total rewards: 1001522.000000, mean rewards: 41817.202505\n",
            "\n",
            "action probability =  [0.4279749478079332, 0.5720250521920668, 0.0]\n",
            "buy =  0.4279749478079332  sell =  0.5720250521920668\n",
            "trade accuracy =  0.929637526652452\n",
            "epoch: 8360, total rewards: 1071079.000000, mean rewards: 44721.461378\n",
            "\n",
            "action probability =  [0.38204592901878914, 0.6179540709812108, 0.0]\n",
            "buy =  0.38204592901878914  sell =  0.6179540709812108\n",
            "trade accuracy =  0.9150743099787686\n",
            "epoch: 8365, total rewards: 1195022.000000, mean rewards: 49896.534447\n",
            "\n",
            "action probability =  [0.3569937369519833, 0.6430062630480167, 0.0]\n",
            "buy =  0.3569937369519833  sell =  0.6430062630480167\n",
            "trade accuracy =  0.8874734607218684\n",
            "epoch: 8370, total rewards: 1173298.000000, mean rewards: 48989.478079\n",
            "\n",
            "action probability =  [0.3883089770354906, 0.6116910229645094, 0.0]\n",
            "buy =  0.3883089770354906  sell =  0.6116910229645094\n",
            "trade accuracy =  0.9212765957446809\n",
            "epoch: 8375, total rewards: 1210909.000000, mean rewards: 50559.874739\n",
            "\n",
            "action probability =  [0.4551148225469729, 0.5448851774530271, 0.0]\n",
            "buy =  0.4551148225469729  sell =  0.5448851774530271\n",
            "trade accuracy =  0.9466950959488273\n",
            "epoch: 8380, total rewards: 1151453.000000, mean rewards: 48077.369520\n",
            "\n",
            "action probability =  [0.5073068893528184, 0.49269311064718163, 0.0]\n",
            "buy =  0.5073068893528184  sell =  0.4926931106471816\n",
            "trade accuracy =  0.8927038626609443\n",
            "epoch: 8385, total rewards: 980801.000000, mean rewards: 40952.025052\n",
            "\n",
            "action probability =  [0.5323590814196242, 0.46764091858037576, 0.0]\n",
            "buy =  0.5323590814196242  sell =  0.4676409185803758\n",
            "trade accuracy =  0.8822269807280514\n",
            "epoch: 8390, total rewards: 967999.000000, mean rewards: 40417.494781\n",
            "\n",
            "action probability =  [0.5031315240083507, 0.4968684759916493, 0.0]\n",
            "buy =  0.5031315240083507  sell =  0.4968684759916493\n",
            "trade accuracy =  0.8991416309012875\n",
            "epoch: 8395, total rewards: 981462.000000, mean rewards: 40979.624217\n",
            "\n",
            "action probability =  [0.4718162839248434, 0.5281837160751566, 0.0]\n",
            "buy =  0.4718162839248434  sell =  0.5281837160751566\n",
            "trade accuracy =  0.9276595744680851\n",
            "epoch: 8400, total rewards: 1141078.000000, mean rewards: 47644.175365\n",
            "\n",
            "action probability =  [0.46346555323590816, 0.5365344467640919, 0.0]\n",
            "buy =  0.46346555323590816  sell =  0.5365344467640918\n",
            "trade accuracy =  0.9320594479830149\n",
            "epoch: 8405, total rewards: 1147901.000000, mean rewards: 47929.060543\n",
            "\n",
            "action probability =  [0.4384133611691023, 0.5615866388308977, 0.0]\n",
            "buy =  0.4384133611691023  sell =  0.5615866388308977\n",
            "trade accuracy =  0.9401709401709402\n",
            "epoch: 8410, total rewards: 1210481.000000, mean rewards: 50542.004175\n",
            "\n",
            "action probability =  [0.4384133611691023, 0.5615866388308977, 0.0]\n",
            "buy =  0.4384133611691023  sell =  0.5615866388308977\n",
            "trade accuracy =  0.9424307036247335\n",
            "epoch: 8415, total rewards: 1214243.000000, mean rewards: 50699.081420\n",
            "\n",
            "action probability =  [0.44467640918580376, 0.5553235908141962, 0.0]\n",
            "buy =  0.44467640918580376  sell =  0.5553235908141962\n",
            "trade accuracy =  0.9404255319148936\n",
            "epoch: 8420, total rewards: 1214443.000000, mean rewards: 50707.432150\n",
            "\n",
            "action probability =  [0.4321503131524008, 0.5678496868475992, 0.0]\n",
            "buy =  0.4321503131524008  sell =  0.5678496868475992\n",
            "trade accuracy =  0.9294871794871795\n",
            "epoch: 8425, total rewards: 1207255.000000, mean rewards: 50407.306889\n",
            "\n",
            "action probability =  [0.4279749478079332, 0.5720250521920668, 0.0]\n",
            "buy =  0.4279749478079332  sell =  0.5720250521920668\n",
            "trade accuracy =  0.9339019189765458\n",
            "epoch: 8430, total rewards: 1208841.000000, mean rewards: 50473.528184\n",
            "\n",
            "action probability =  [0.4112734864300626, 0.5887265135699373, 0.0]\n",
            "buy =  0.4112734864300626  sell =  0.5887265135699373\n",
            "trade accuracy =  0.9275053304904051\n",
            "epoch: 8435, total rewards: 1197439.000000, mean rewards: 49997.453027\n",
            "\n",
            "action probability =  [0.40501043841336115, 0.5949895615866388, 0.0]\n",
            "buy =  0.40501043841336115  sell =  0.5949895615866388\n",
            "trade accuracy =  0.9276595744680851\n",
            "epoch: 8440, total rewards: 1211800.000000, mean rewards: 50597.077244\n",
            "\n",
            "action probability =  [0.42379958246346555, 0.5762004175365344, 0.0]\n",
            "buy =  0.42379958246346555  sell =  0.5762004175365345\n",
            "trade accuracy =  0.9468085106382979\n",
            "epoch: 8445, total rewards: 1219857.000000, mean rewards: 50933.486430\n",
            "\n",
            "action probability =  [0.4488517745302714, 0.5511482254697286, 0.0]\n",
            "buy =  0.4488517745302714  sell =  0.5511482254697286\n",
            "trade accuracy =  0.9530916844349681\n",
            "epoch: 8450, total rewards: 1214587.000000, mean rewards: 50713.444676\n",
            "\n",
            "action probability =  [0.44467640918580376, 0.5553235908141962, 0.0]\n",
            "buy =  0.44467640918580376  sell =  0.5553235908141962\n",
            "trade accuracy =  0.9532908704883227\n",
            "epoch: 8455, total rewards: 1215030.000000, mean rewards: 50731.941545\n",
            "\n",
            "action probability =  [0.4405010438413361, 0.5594989561586639, 0.0]\n",
            "buy =  0.4405010438413361  sell =  0.5594989561586639\n",
            "trade accuracy =  0.9469214437367304\n",
            "epoch: 8460, total rewards: 1211557.000000, mean rewards: 50586.931106\n",
            "\n",
            "action probability =  [0.4613778705636743, 0.5386221294363257, 0.0]\n",
            "buy =  0.4613778705636743  sell =  0.5386221294363257\n",
            "trade accuracy =  0.9552238805970149\n",
            "epoch: 8465, total rewards: 1214736.000000, mean rewards: 50719.665971\n",
            "\n",
            "action probability =  [0.48434237995824636, 0.5156576200417536, 0.0]\n",
            "buy =  0.48434237995824636  sell =  0.5156576200417536\n",
            "trade accuracy =  0.9425531914893617\n",
            "epoch: 8470, total rewards: 1192447.000000, mean rewards: 49789.018789\n",
            "\n",
            "action probability =  [0.4968684759916493, 0.5031315240083507, 0.0]\n",
            "buy =  0.4968684759916493  sell =  0.5031315240083507\n",
            "trade accuracy =  0.9363057324840764\n",
            "epoch: 8475, total rewards: 1183420.000000, mean rewards: 49412.108559\n",
            "\n",
            "action probability =  [0.4801670146137787, 0.5198329853862212, 0.0]\n",
            "buy =  0.4801670146137787  sell =  0.5198329853862214\n",
            "trade accuracy =  0.9491525423728814\n",
            "epoch: 8480, total rewards: 1193522.000000, mean rewards: 49833.903967\n",
            "\n",
            "action probability =  [0.4488517745302714, 0.5511482254697286, 0.0]\n",
            "buy =  0.4488517745302714  sell =  0.5511482254697286\n",
            "trade accuracy =  0.951271186440678\n",
            "epoch: 8485, total rewards: 1204275.000000, mean rewards: 50282.881002\n",
            "\n",
            "action probability =  [0.4154488517745303, 0.5845511482254697, 0.0]\n",
            "buy =  0.4154488517745303  sell =  0.5845511482254697\n",
            "trade accuracy =  0.9256900212314225\n",
            "epoch: 8490, total rewards: 1191106.000000, mean rewards: 49733.027140\n",
            "\n",
            "action probability =  [0.4112734864300626, 0.5887265135699373, 0.0]\n",
            "buy =  0.4112734864300626  sell =  0.5887265135699373\n",
            "trade accuracy =  0.9193205944798302\n",
            "epoch: 8495, total rewards: 1186609.000000, mean rewards: 49545.260960\n",
            "\n",
            "action probability =  [0.40083507306889354, 0.5991649269311065, 0.0]\n",
            "buy =  0.40083507306889354  sell =  0.5991649269311065\n",
            "trade accuracy =  0.9194915254237288\n",
            "epoch: 8500, total rewards: 1205063.000000, mean rewards: 50315.782881\n",
            "\n",
            "action probability =  [0.40083507306889354, 0.5991649269311065, 0.0]\n",
            "buy =  0.40083507306889354  sell =  0.5991649269311065\n",
            "trade accuracy =  0.9276595744680851\n",
            "epoch: 8505, total rewards: 1205842.000000, mean rewards: 50348.308977\n",
            "\n",
            "action probability =  [0.40083507306889354, 0.5991649269311065, 0.0]\n",
            "buy =  0.40083507306889354  sell =  0.5991649269311065\n",
            "trade accuracy =  0.9319148936170213\n",
            "epoch: 8510, total rewards: 1208050.000000, mean rewards: 50440.501044\n",
            "\n",
            "action probability =  [0.42379958246346555, 0.5762004175365344, 0.0]\n",
            "buy =  0.42379958246346555  sell =  0.5762004175365345\n",
            "trade accuracy =  0.9382978723404255\n",
            "epoch: 8515, total rewards: 1206975.000000, mean rewards: 50395.615866\n",
            "\n",
            "action probability =  [0.4801670146137787, 0.5198329853862212, 0.0]\n",
            "buy =  0.4801670146137787  sell =  0.5198329853862214\n",
            "trade accuracy =  0.9293361884368309\n",
            "epoch: 8520, total rewards: 1191861.000000, mean rewards: 49764.551148\n",
            "\n",
            "action probability =  [0.524008350730689, 0.4759916492693111, 0.0]\n",
            "buy =  0.524008350730689  sell =  0.47599164926931103\n",
            "trade accuracy =  0.8763326226012793\n",
            "epoch: 8525, total rewards: 1018145.000000, mean rewards: 42511.273486\n",
            "\n",
            "action probability =  [0.5135699373695198, 0.48643006263048016, 0.0]\n",
            "buy =  0.5135699373695198  sell =  0.48643006263048016\n",
            "trade accuracy =  0.8886509635974305\n",
            "epoch: 8530, total rewards: 945568.000000, mean rewards: 39480.918580\n",
            "\n",
            "action probability =  [0.4968684759916493, 0.5031315240083507, 0.0]\n",
            "buy =  0.4968684759916493  sell =  0.5031315240083507\n",
            "trade accuracy =  0.9211087420042644\n",
            "epoch: 8535, total rewards: 1103260.000000, mean rewards: 46065.135699\n",
            "\n",
            "action probability =  [0.46764091858037576, 0.5323590814196242, 0.0]\n",
            "buy =  0.46764091858037576  sell =  0.5323590814196242\n",
            "trade accuracy =  0.9382978723404255\n",
            "epoch: 8540, total rewards: 1198077.000000, mean rewards: 50024.091858\n",
            "\n",
            "action probability =  [0.4509394572025052, 0.5490605427974948, 0.0]\n",
            "buy =  0.4509394572025052  sell =  0.5490605427974948\n",
            "trade accuracy =  0.945031712473573\n",
            "epoch: 8545, total rewards: 1202663.000000, mean rewards: 50215.574113\n",
            "\n",
            "action probability =  [0.4196242171189979, 0.5803757828810021, 0.0]\n",
            "buy =  0.4196242171189979  sell =  0.5803757828810021\n",
            "trade accuracy =  0.9320594479830149\n",
            "epoch: 8550, total rewards: 1196479.000000, mean rewards: 49957.369520\n",
            "\n",
            "action probability =  [0.3924843423799583, 0.6075156576200418, 0.0]\n",
            "buy =  0.3924843423799583  sell =  0.6075156576200418\n",
            "trade accuracy =  0.9258474576271186\n",
            "epoch: 8555, total rewards: 1205875.000000, mean rewards: 50349.686848\n",
            "\n",
            "action probability =  [0.3924843423799583, 0.6075156576200418, 0.0]\n",
            "buy =  0.3924843423799583  sell =  0.6075156576200418\n",
            "trade accuracy =  0.9300847457627118\n",
            "epoch: 8560, total rewards: 1209553.000000, mean rewards: 50503.256785\n",
            "\n",
            "action probability =  [0.3966597077244259, 0.6033402922755741, 0.0]\n",
            "buy =  0.3966597077244259  sell =  0.6033402922755742\n",
            "trade accuracy =  0.934322033898305\n",
            "epoch: 8565, total rewards: 1216134.000000, mean rewards: 50778.037578\n",
            "\n",
            "action probability =  [0.430062630480167, 0.569937369519833, 0.0]\n",
            "buy =  0.430062630480167  sell =  0.569937369519833\n",
            "trade accuracy =  0.9575371549893843\n",
            "epoch: 8570, total rewards: 1221111.000000, mean rewards: 50985.845511\n",
            "\n",
            "action probability =  [0.4509394572025052, 0.5490605427974948, 0.0]\n",
            "buy =  0.4509394572025052  sell =  0.5490605427974948\n",
            "trade accuracy =  0.9509594882729211\n",
            "epoch: 8575, total rewards: 1218914.000000, mean rewards: 50894.112735\n",
            "\n",
            "action probability =  [0.453027139874739, 0.5469728601252609, 0.0]\n",
            "buy =  0.453027139874739  sell =  0.546972860125261\n",
            "trade accuracy =  0.9382978723404255\n",
            "epoch: 8580, total rewards: 1215067.000000, mean rewards: 50733.486430\n",
            "\n",
            "action probability =  [0.46555323590814196, 0.534446764091858, 0.0]\n",
            "buy =  0.46555323590814196  sell =  0.534446764091858\n",
            "trade accuracy =  0.9340425531914893\n",
            "epoch: 8585, total rewards: 1211383.000000, mean rewards: 50579.665971\n",
            "\n",
            "action probability =  [0.4613778705636743, 0.5386221294363257, 0.0]\n",
            "buy =  0.4613778705636743  sell =  0.5386221294363257\n",
            "trade accuracy =  0.9381663113006397\n",
            "epoch: 8590, total rewards: 1214449.000000, mean rewards: 50707.682672\n",
            "\n",
            "action probability =  [0.4759916492693111, 0.524008350730689, 0.0]\n",
            "buy =  0.4759916492693111  sell =  0.524008350730689\n",
            "trade accuracy =  0.9425531914893617\n",
            "epoch: 8595, total rewards: 1212445.000000, mean rewards: 50624.008351\n",
            "\n",
            "action probability =  [0.4718162839248434, 0.5281837160751566, 0.0]\n",
            "buy =  0.4718162839248434  sell =  0.5281837160751566\n",
            "trade accuracy =  0.9384288747346072\n",
            "epoch: 8600, total rewards: 1210270.000000, mean rewards: 50533.194154\n",
            "\n",
            "action probability =  [0.46555323590814196, 0.534446764091858, 0.0]\n",
            "buy =  0.46555323590814196  sell =  0.534446764091858\n",
            "trade accuracy =  0.940552016985138\n",
            "epoch: 8605, total rewards: 1211726.000000, mean rewards: 50593.987474\n",
            "\n",
            "action probability =  [0.4697286012526096, 0.5302713987473904, 0.0]\n",
            "buy =  0.4697286012526096  sell =  0.5302713987473904\n",
            "trade accuracy =  0.9404255319148936\n",
            "epoch: 8610, total rewards: 1206935.000000, mean rewards: 50393.945720\n",
            "\n",
            "action probability =  [0.4697286012526096, 0.5302713987473904, 0.0]\n",
            "buy =  0.4697286012526096  sell =  0.5302713987473904\n",
            "trade accuracy =  0.9424307036247335\n",
            "epoch: 8615, total rewards: 1211226.000000, mean rewards: 50573.110647\n",
            "\n",
            "action probability =  [0.46555323590814196, 0.534446764091858, 0.0]\n",
            "buy =  0.46555323590814196  sell =  0.534446764091858\n",
            "trade accuracy =  0.9511677282377919\n",
            "epoch: 8620, total rewards: 1169754.000000, mean rewards: 48841.503132\n",
            "\n",
            "action probability =  [0.46555323590814196, 0.534446764091858, 0.0]\n",
            "buy =  0.46555323590814196  sell =  0.534446764091858\n",
            "trade accuracy =  0.9446808510638298\n",
            "epoch: 8625, total rewards: 1104172.000000, mean rewards: 46103.215031\n",
            "\n",
            "action probability =  [0.4154488517745303, 0.5845511482254697, 0.0]\n",
            "buy =  0.4154488517745303  sell =  0.5845511482254697\n",
            "trade accuracy =  0.9530916844349681\n",
            "epoch: 8630, total rewards: 1175490.000000, mean rewards: 49081.002088\n",
            "\n",
            "action probability =  [0.40292275574112735, 0.5970772442588727, 0.0]\n",
            "buy =  0.40292275574112735  sell =  0.5970772442588727\n",
            "trade accuracy =  0.9424307036247335\n",
            "epoch: 8635, total rewards: 1166726.000000, mean rewards: 48715.073069\n",
            "\n",
            "action probability =  [0.40083507306889354, 0.5991649269311065, 0.0]\n",
            "buy =  0.40083507306889354  sell =  0.5991649269311065\n",
            "trade accuracy =  0.9404255319148936\n",
            "epoch: 8640, total rewards: 1166620.000000, mean rewards: 48710.647182\n",
            "\n",
            "action probability =  [0.4196242171189979, 0.5803757828810021, 0.0]\n",
            "buy =  0.4196242171189979  sell =  0.5803757828810021\n",
            "trade accuracy =  0.9491525423728814\n",
            "epoch: 8645, total rewards: 1164504.000000, mean rewards: 48622.296451\n",
            "\n",
            "action probability =  [0.4592901878914405, 0.5407098121085595, 0.0]\n",
            "buy =  0.4592901878914405  sell =  0.5407098121085595\n",
            "trade accuracy =  0.9363057324840764\n",
            "epoch: 8650, total rewards: 1090180.000000, mean rewards: 45518.997912\n",
            "\n",
            "action probability =  [0.4801670146137787, 0.5198329853862212, 0.0]\n",
            "buy =  0.4801670146137787  sell =  0.5198329853862214\n",
            "trade accuracy =  0.9320594479830149\n",
            "epoch: 8655, total rewards: 1086196.000000, mean rewards: 45352.651357\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rgHeJaUBol3i",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "    def _select_action(self, state, i): \n",
        "      prediction,q = self.sess.run([self.policy,self.dqn], feed_dict={self.state: [state]})\n",
        "      prediction,q = prediction[0],q[0]\n",
        "\n",
        "      q = np.abs(q) / np.sum(q)\n",
        "      noise = 0.2 if (i+1) % 5 != 0 else 0.\n",
        "      prediction += 0.2 * np.random.randn(2)\n",
        "      q += 0.2 * np.random.randn(2)\n",
        "      prediction = np.clip(prediction,-1,1)\n",
        "      q = np.clip(q,-1,1)\n",
        "      if np.random.rand() < self.epsilon and (i+1) % 5 != 0:\n",
        "        q += prediction\n",
        "        q = softmax(q)\n",
        "        exp = np.exp(q)\n",
        "        exp /= sum(exp)\n",
        "        action = np.random.choice(range(2), p=exp)\n",
        "      else:\n",
        "        q += prediction\n",
        "        action = np.argmax(q)\n",
        "      self.pred = prediction\n",
        "      return action"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PPD5ArIzpNTN",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "agent.pred"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}